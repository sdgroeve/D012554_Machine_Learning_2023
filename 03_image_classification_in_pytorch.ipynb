{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdgroeve/D012554_Machine_Learning_2023/blob/main/03_image_classification_in_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "request = requests.get(\"https://raw.githubusercontent.com/sdgroeve/D012554_Machine_Learning_2023/main/utils/utils.py\")\n",
        "with open(\"utils.py\", \"wb\") as f:\n",
        "  f.write(request.content)\n",
        "\n",
        "from utils import plot_decision_boundary"
      ],
      "metadata": {
        "id": "3lBi6JV6xhXZ",
        "cellView": "form"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!pip install tqdm\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU4n2UT6ZsWA",
        "outputId": "2e5411a3-ce9c-479b-aace-8b115209fd5d",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgYkrRCRec0r"
      },
      "source": [
        "# 3. Image Classification in PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZT_ikDC-ec0w",
        "outputId": "2fbed6d6-ec79-47db-91d2-495328992695"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 139
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "torch.manual_seed(46)\n",
        "\n",
        "# Check PyTorch version\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci_-geIdec0w"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "We will use [`torchvision`](https://pytorch.org/vision/stable/index.html) to load our dataset. This library contains datasets, model architectures and image transformations often used for computer vision prediction tasks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import torchvision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
        "print(f\"torchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZo_wyZRbMmd",
        "outputId": "7ceee695-1894-4495-db1a-44ffefd02a66"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torchvision version: 0.17.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train our model on the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, made by Zalando Research, that contains grayscale images of 10 different kinds of clothing.\n",
        "\n",
        "To download it, we provide the following parameters:\n",
        "* `root: str` - which folder do you want to download the data to?\n",
        "* `train: Bool` - do you want the training or test split?\n",
        "* `download: Bool` - should the data be downloaded?\n",
        "* `transform: torchvision.transforms` - what transformations would you like to do on the data?\n",
        "* `target_transform` - you can transform the targets (labels) if you like too.\n",
        "\n",
        "Let's load the data using torchvision."
      ],
      "metadata": {
        "id": "VFZFvc4kdVtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train set\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True, # get training data\n",
        "    download=True,\n",
        "    transform=ToTensor(), # images come as PIL format, we want to turn them into Torch tensors\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "# test set\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # get test data\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "-emcG7FnbWjB"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train set\n",
        "train_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True, # get training data\n",
        "    download=True,\n",
        "    transform=ToTensor(), # images come as PIL format, we want to turn them into Torch tensors\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "# test set\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False, # get test data\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "3pOflXfQDaAX",
        "outputId": "4df011f2-3069-474c-8134-08770f67033d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:06<00:00, 26677574.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 60.000 training images and 10.000 testing images."
      ],
      "metadata": {
        "id": "BJ7J5Z4MVCiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0V1_9EuU_lu",
        "outputId": "9b08829c-6736-44d4-917a-11889a65696d"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the classes:"
      ],
      "metadata": {
        "id": "Gvq4ZGKCVvco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "print(len(class_names))\n",
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDL13CqaVuXx",
        "outputId": "8fa01c49-c089-48a0-92db-f75efed89b37"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['airplane',\n",
              " 'automobile',\n",
              " 'bird',\n",
              " 'cat',\n",
              " 'deer',\n",
              " 'dog',\n",
              " 'frog',\n",
              " 'horse',\n",
              " 'ship',\n",
              " 'truck']"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code splits the training data into a training data and validation data."
      ],
      "metadata": {
        "id": "YvApnWeqVVgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data, validation_data = torch.utils.data.random_split(train_data, [50000, 10000])\n",
        "train_data, validation_data = torch.utils.data.random_split(train_data, [40000, 10000])"
      ],
      "metadata": {
        "id": "ZPUueophU1Bc"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the first image (data point) in the train set."
      ],
      "metadata": {
        "id": "_EoNPyLR7Lqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AHLTkPu7TCn",
        "outputId": "1409c68c-6fad-4163-8825-9e932dd652ff"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.9922, 0.9765, 0.9804,  ..., 0.9216, 0.8745, 0.8471],\n",
              "          [0.9882, 0.9765, 0.9804,  ..., 0.8627, 0.8431, 0.8196],\n",
              "          [0.9922, 0.9843, 0.9922,  ..., 0.9255, 0.9098, 0.8902],\n",
              "          ...,\n",
              "          [0.5765, 0.6078, 0.4863,  ..., 0.2118, 0.2902, 0.2824],\n",
              "          [0.3216, 0.1804, 0.1059,  ..., 0.2118, 0.2510, 0.2471],\n",
              "          [0.0824, 0.0745, 0.0902,  ..., 0.2000, 0.2353, 0.2314]],\n",
              " \n",
              "         [[1.0000, 0.9961, 1.0000,  ..., 0.9804, 0.9843, 0.9804],\n",
              "          [1.0000, 0.9922, 0.9961,  ..., 0.9765, 0.9804, 0.9804],\n",
              "          [1.0000, 0.9961, 0.9961,  ..., 0.9882, 0.9882, 0.9843],\n",
              "          ...,\n",
              "          [0.5765, 0.6000, 0.4745,  ..., 0.2980, 0.3922, 0.3922],\n",
              "          [0.3333, 0.1961, 0.1255,  ..., 0.3176, 0.3725, 0.3725],\n",
              "          [0.0980, 0.0863, 0.0980,  ..., 0.3176, 0.3647, 0.3608]],\n",
              " \n",
              "         [[0.9882, 0.9725, 0.9765,  ..., 0.9725, 0.9647, 0.9725],\n",
              "          [0.9843, 0.9725, 0.9725,  ..., 0.9647, 0.9608, 0.9608],\n",
              "          [0.9882, 0.9765, 0.9765,  ..., 0.9765, 0.9725, 0.9725],\n",
              "          ...,\n",
              "          [0.5451, 0.5451, 0.4353,  ..., 0.2902, 0.4196, 0.4275],\n",
              "          [0.3137, 0.1647, 0.0941,  ..., 0.3020, 0.3961, 0.4039],\n",
              "          [0.0941, 0.0745, 0.0863,  ..., 0.2980, 0.3843, 0.3882]]]),\n",
              " 8)"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each image is a tensor with the following shape."
      ],
      "metadata": {
        "id": "fxAv20e98ZS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08DYvwyA7cVr",
        "outputId": "2ffa0a13-047a-4d87-e615-a0e9927affd4"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape of the image tensor is `[1, 28, 28]` or more specifically:\n",
        "\n",
        "```\n",
        "[color_channels=1, height=28, width=28]\n",
        "```\n",
        "\n",
        "Having `color_channels=1` means the image is grayscale."
      ],
      "metadata": {
        "id": "flEpM21P8dsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the first image in the train set."
      ],
      "metadata": {
        "id": "XTpz09s6JC0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#image, label = train_data[0]\n",
        "\n",
        "#plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "#plt.title(class_names[label])"
      ],
      "metadata": {
        "id": "wLAkhtiRImaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming train_data is your dataset\n",
        "image, label = train_data[1] # Get the first image and its label\n",
        "\n",
        "# Permute the tensor from [C, H, W] to [H, W, C]\n",
        "image = image.permute(1, 2, 0)\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.title(f\"Label: {label}\")\n",
        "plt.axis('off') # Hide the axes\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6wyzIPCgHvfb",
        "outputId": "76a337e9-003a-4da5-854f-40e5d3fc732a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot some more."
      ],
      "metadata": {
        "id": "jLlYNGIZJdgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "fig = plt.figure(figsize=(14, 16))\n",
        "rows, cols = 8, 8\n",
        "for i in range(1, rows * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    img, label = train_data[random_idx]\n",
        "    fig.add_subplot(rows, cols, i)\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    plt.title(class_names[label])\n",
        "    plt.axis(False);\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Kj5IJPx2JgDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up till now we have updated the modelparameters once in each epoch. For large dataset this is highly inefficient.\n",
        "\n",
        "It is better to partition the train set into smaller chucks of data, called **batches** (or **minibatches** as this approach works best for very small batch sizes), and update the modelparameters after each batch.  \n",
        "\n",
        "A minibatch size of [32 data points](https://twitter.com/ylecun/status/989610208497360896?s=20&t=N96J_jotN--PYuJk2WcjMw) is a good place to start for most tasks.\n",
        "\n",
        "The PyTorch `DataLoader` class makes using minibatches really easy."
      ],
      "metadata": {
        "id": "xmT4PrIGQAWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(validation_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "_aSQcQfvSG5t"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eFsorRHec00"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Since each image is a 2-dimensional (3 if you count the channel), we first **flatten** the image to just 1 dimension to prepare the input for the input layer of our neural network, i.e. each pixel is one feature."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
        "\n",
        "# Get a single sample\n",
        "x, y = train_data[0]\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x) # perform forward pass\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")\n",
        "\n",
        "# Try uncommenting below and see what happens\n",
        "#print(x)\n",
        "#print(output)"
      ],
      "metadata": {
        "id": "2lk2KEQNYpMx",
        "outputId": "b537da13-7ce5-4c18-d013-879b696a2b64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before flattening: torch.Size([3, 32, 32]) -> [color_channels, height, width]\n",
            "Shape after flattening: torch.Size([3, 1024]) -> [color_channels, height*width]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "jhcUJBFuec00"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        num_neurons_layer_2 = 10\n",
        "\n",
        "        self.layer_1 = nn.Linear(in_features=input_dim, out_features=num_neurons_layer_2)\n",
        "        self.layer_2 = nn.Linear(in_features=num_neurons_layer_2, out_features=output_dim)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.layer_1(x))\n",
        "        x = self.layer_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `__init()__`\n",
        "\n",
        "Our neural network has two linear layers. The first layer `layer_1` has `input_dim` (the number of features in our dataset) input features that form the **input layer**. It has `num_neurons_layer_2` output features that form the **hidden layer** where these features are typically called **hidden neurons**.\n",
        "\n",
        "The second layer `layer_2` has `num_neurons_layer_2` input features (neurons) and `output_dim` (which equals to 1 for two-class classification) output features, the **output layer**.\n",
        "\n",
        "An example of this model architecture with `num_neurons_layer_2 = 6` can be seen [here](https://playground.tensorflow.org/#activation=sigmoid&batchSize=30&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=6&seed=0.86658&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false).\n",
        "\n",
        "We will use the Rectified Linear Unit (ReLU) activation function in the hidden layer. In the output layer we use the sigmoid function (through `BCEWithLogitsLoss`, so we not to explicitly apply the sigmoid function during inference (see notebook about logistic regression)).\n",
        "\n",
        "Next, we create an instance of the class `NeuralNetwork`."
      ],
      "metadata": {
        "id": "Fj8ygxXdET-y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "CsEKA3A_ec01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f757f949-1cdc-4884-beab-57dc82306985"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('layer_1.weight',\n",
              "              tensor([[-0.0037, -0.0049, -0.0166,  ..., -0.0084,  0.0137, -0.0120],\n",
              "                      [ 0.0131,  0.0084,  0.0118,  ..., -0.0021,  0.0081,  0.0172],\n",
              "                      [ 0.0128,  0.0097, -0.0046,  ...,  0.0167, -0.0103, -0.0101],\n",
              "                      ...,\n",
              "                      [ 0.0126,  0.0058, -0.0017,  ...,  0.0012,  0.0100, -0.0154],\n",
              "                      [ 0.0055, -0.0093, -0.0080,  ...,  0.0068,  0.0091, -0.0009],\n",
              "                      [-0.0065,  0.0105, -0.0131,  ..., -0.0027,  0.0007,  0.0020]])),\n",
              "             ('layer_1.bias',\n",
              "              tensor([ 0.0118, -0.0086,  0.0102, -0.0067,  0.0111,  0.0043, -0.0083,  0.0091,\n",
              "                      -0.0014,  0.0167])),\n",
              "             ('layer_2.weight',\n",
              "              tensor([[ 0.2831,  0.2765, -0.2879,  0.1543,  0.3160, -0.2183,  0.2131, -0.2246,\n",
              "                       -0.1957,  0.2564],\n",
              "                      [ 0.2432,  0.1304,  0.3055,  0.2690, -0.2618, -0.3152,  0.0745,  0.0936,\n",
              "                       -0.3031, -0.1696],\n",
              "                      [ 0.3119, -0.1700,  0.2916, -0.2965, -0.1166, -0.3141,  0.2296, -0.2463,\n",
              "                        0.1620,  0.0109],\n",
              "                      [ 0.2119, -0.0406, -0.0822, -0.1579,  0.0377,  0.1652, -0.1497,  0.0102,\n",
              "                       -0.2148, -0.0725],\n",
              "                      [ 0.1398,  0.0657,  0.2911,  0.0113,  0.0151, -0.1608,  0.0294, -0.0023,\n",
              "                        0.2961, -0.0876],\n",
              "                      [ 0.2040,  0.1841, -0.0439,  0.2472, -0.0746, -0.3116,  0.0511,  0.0973,\n",
              "                       -0.0538,  0.2858],\n",
              "                      [ 0.1231,  0.3019,  0.2104,  0.1057, -0.0031, -0.2930, -0.0609,  0.2569,\n",
              "                        0.0435, -0.2979],\n",
              "                      [ 0.1727, -0.1820, -0.1021, -0.1994, -0.2912,  0.2682, -0.1636, -0.0038,\n",
              "                       -0.1738,  0.1741],\n",
              "                      [ 0.3047,  0.1757,  0.0643,  0.0430,  0.0594, -0.0330, -0.0920,  0.0218,\n",
              "                        0.1738,  0.0455],\n",
              "                      [ 0.1170, -0.1074, -0.1498,  0.1833, -0.1168, -0.1051,  0.1731, -0.2499,\n",
              "                       -0.0111,  0.1754]])),\n",
              "             ('layer_2.bias',\n",
              "              tensor([-0.2853, -0.2083, -0.2622, -0.2100,  0.0082, -0.3109, -0.2907,  0.2857,\n",
              "                       0.1292, -0.2162]))])"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ],
      "source": [
        "# Two inputs x_1 and x_2\n",
        "#input_dim = 784\n",
        "input_dim = 32*32*3\n",
        "# Single binary output\n",
        "output_dim = 10\n",
        "\n",
        "# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\n",
        "model = NeuralNetwork(input_dim, output_dim)\n",
        "\n",
        "model.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKdLN7nuheb"
      },
      "source": [
        "### `forward()`\n",
        "\n",
        "The `forward()` method applies the neural network to the provided feature vectors. Here we see that the data is first passed through `layer_1`, then through the ReLU activations that then pass through `layer_2`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(device)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "oVhnxzf1Q1JX",
        "outputId": "25984607-f602-4f11-9e60-bd0fa7aa7e77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (layer_1): Linear(in_features=3072, out_features=10, bias=True)\n",
              "  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "\n",
        "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)"
      ],
      "metadata": {
        "id": "gsHsfzU9QqyF"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ITlZgU5ec02"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "model.eval()\n",
        "test_acc = 0, 0\n",
        "with torch.inference_mode():\n",
        "    for X, y in test_dataloader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred_logits = model(X)\n",
        "        #y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "        # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "        metric.update(y_pred_logits, y)\n",
        "\"\"\"\n",
        "\n",
        "model.train()\n",
        "\n",
        "test_acc = metric.compute()\n",
        "\n",
        "## Print out what's happening\n",
        "print(f\"\\nTest acc: {test_acc:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD8pnhJUyZUT"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "We use `BCEWithLogitsLoss` as the loss function and SGD, `torch.optim.SGD(params, lr)` as the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "P3T7hpNPec03"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "#the loss function\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "#the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GHdllgi9sfjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFcKCsPcRfnA"
      },
      "source": [
        "Now we can create and run our training and validation loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1DfhyJ7ec03",
        "outputId": "07009df6-8201-4b4d-a0a4-7ff2c2102f5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.03367 | Validation loss: 1.93216, Validation acc: 0.28%\n",
            "Train loss: 1.87003 | Validation loss: 1.84432, Validation acc: 0.34%\n",
            "Train loss: 1.82878 | Validation loss: 1.86982, Validation acc: 0.33%\n",
            "Train loss: 1.80452 | Validation loss: 1.80249, Validation acc: 0.36%\n",
            "Train loss: 1.78432 | Validation loss: 1.77878, Validation acc: 0.36%\n",
            "Train loss: 1.76936 | Validation loss: 1.77766, Validation acc: 0.37%\n",
            "Train loss: 1.75458 | Validation loss: 1.77404, Validation acc: 0.36%\n",
            "Train loss: 1.74301 | Validation loss: 1.77847, Validation acc: 0.37%\n",
            "Train loss: 1.73096 | Validation loss: 1.74389, Validation acc: 0.38%\n",
            "Train loss: 1.71883 | Validation loss: 1.75109, Validation acc: 0.37%\n"
          ]
        }
      ],
      "source": [
        "#number of times we iterate trough the train set\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      #step 1\n",
        "      predictions_train = model(X)\n",
        "\n",
        "      #step 2\n",
        "      loss = loss_func(predictions_train, y)\n",
        "      train_loss += loss\n",
        "\n",
        "      #step 3\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #step 4\n",
        "      loss.backward()\n",
        "\n",
        "      #step 5\n",
        "      optimizer.step()\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.inference_mode():\n",
        "        for X, y in validation_dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            y_pred_logits = model(X)\n",
        "            y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "            # 2. Calculate loss (accumatively)\n",
        "            val_loss += loss_func(y_pred_logits, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            metric.update(y_pred_logits, y)\n",
        "\n",
        "        val_loss /= len(validation_dataloader)\n",
        "\n",
        "    val_acc = metric.compute()\n",
        "    metric.reset()\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"Train loss: {train_loss:.5f} | Validation loss: {val_loss:.5f}, Validation acc: {val_acc:.2f}%\")\n",
        "\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_acc = 0\n",
        "with torch.inference_mode():\n",
        "    for X, y in test_dataloader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred_logits = model(X)\n",
        "        #y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "        # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "        metric.update(y_pred_logits, y)\n",
        "\n",
        "model.train()\n",
        "\n",
        "test_acc = metric.compute()\n",
        "\n",
        "## Print out what's happening\n",
        "print(f\"\\nTest acc: {test_acc:.2f}%\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CS7DuRIwj8m",
        "outputId": "a4112a40-318c-40db-df1b-12db4328ee75"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test acc: 0.38%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture copying TinyVGG from:\n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3, # how big is the square that's going over the image?\n",
        "                      stride=1, # default\n",
        "                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2) # default stride value is same as kernel_size\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # Where did this in_features shape come from?\n",
        "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "            #nn.Linear(in_features=hidden_units*7*7,\n",
        "            #          out_features=output_shape)\n",
        "            nn.Linear(in_features=hidden_units*8*8,\n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.block_2(x)\n",
        "        # print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        # print(x.shape)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kZtwC9wx6t46"
      },
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = CNN(input_shape=1,\n",
        "#    hidden_units=10,\n",
        "#    output_shape=len(class_names)).to(device)\n",
        "\n",
        "model = CNN(input_shape=3,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names)).to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6w9ATp37VAz",
        "outputId": "8f7eef33-8f5d-4d25-ae51-e91c913cf61a"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (block_1): Sequential(\n",
              "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (block_2): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=640, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "#the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Ee3_pM2A7673"
      },
      "execution_count": 171,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of times we iterate trough the train set\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      #step 1\n",
        "      predictions_train = model(X)\n",
        "\n",
        "      #step 2\n",
        "      loss = loss_func(predictions_train, y)\n",
        "      train_loss += loss\n",
        "\n",
        "      #step 3\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #step 4\n",
        "      loss.backward()\n",
        "\n",
        "      #step 5\n",
        "      optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.inference_mode():\n",
        "        for X, y in validation_dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            y_pred_logits = model(X)\n",
        "            y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "            # 2. Calculate loss (accumatively)\n",
        "            val_loss += loss_func(y_pred_logits, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            metric.update(y_pred_logits, y)\n",
        "\n",
        "        val_loss /= len(validation_dataloader)\n",
        "\n",
        "    val_acc = metric.compute()\n",
        "\n",
        "    metric.reset()\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"Train loss: {train_loss:.5f} | Validation loss: {val_loss:.5f}, Validation acc: {val_acc:.2f}%\")\n",
        "\n",
        "    model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h_8lqM77fl-",
        "outputId": "0680d6e3-3f93-442a-c6f1-60208032d6a6"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2538.82471 | Validation loss: 1.65511, Validation acc: 0.39%\n",
            "Train loss: 1897.58350 | Validation loss: 1.38974, Validation acc: 0.50%\n",
            "Train loss: 1683.78821 | Validation loss: 1.43563, Validation acc: 0.51%\n",
            "Train loss: 1585.41504 | Validation loss: 1.26269, Validation acc: 0.55%\n",
            "Train loss: 1522.01282 | Validation loss: 1.21259, Validation acc: 0.58%\n",
            "Train loss: 1484.95239 | Validation loss: 1.21559, Validation acc: 0.57%\n",
            "Train loss: 1457.41797 | Validation loss: 1.23613, Validation acc: 0.57%\n",
            "Train loss: 1434.15930 | Validation loss: 1.18507, Validation acc: 0.59%\n",
            "Train loss: 1420.91638 | Validation loss: 1.21841, Validation acc: 0.58%\n",
            "Train loss: 1405.85718 | Validation loss: 1.16001, Validation acc: 0.59%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing predictions and evaluating the model\n"
      ],
      "metadata": {
        "id": "e0MESO4WgNQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_acc = 0\n",
        "with torch.inference_mode():\n",
        "    for X, y in test_dataloader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred_logits = model(X)\n",
        "        #y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "        # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "        metric.update(y_pred_logits, y)\n",
        "\n",
        "model.train()\n",
        "\n",
        "test_acc = metric.compute()\n",
        "\n",
        "## Print out what's happening\n",
        "print(f\"\\nTest acc: {test_acc:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "X1saE3rqbrS3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "495ea520-b09a-4ea3-a956-55692844f78d"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test acc: 0.59%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkSk6FTvath3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "01_pytorch_workflow.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "3fbe1355223f7b2ffc113ba3ade6a2b520cadace5d5ec3e828c83ce02eb221bf"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}