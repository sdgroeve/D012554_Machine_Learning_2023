{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdgroeve/D012554_Machine_Learning_2023/blob/main/03_image_classification_in_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "request = requests.get(\"https://raw.githubusercontent.com/sdgroeve/D012554_Machine_Learning_2023/main/utils/utils.py\")\n",
        "with open(\"utils.py\", \"wb\") as f:\n",
        "  f.write(request.content)\n",
        "\n",
        "from utils import plot_decision_boundary"
      ],
      "metadata": {
        "id": "3lBi6JV6xhXZ",
        "cellView": "form"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!pip install tqdm\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU4n2UT6ZsWA",
        "outputId": "ca6387c1-44e1-48fc-b761-54eab3c8e0a6",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgYkrRCRec0r"
      },
      "source": [
        "# 3. Image Classification in PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 182,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZT_ikDC-ec0w",
        "outputId": "9925674d-4669-4dcc-e629-1ed80650a6d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 182
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "torch.manual_seed(46)\n",
        "\n",
        "# Check PyTorch version\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci_-geIdec0w"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "We will use [`torchvision`](https://pytorch.org/vision/stable/index.html) to load our dataset. This library contains datasets, model architectures and image transformations often used for computer vision prediction tasks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import torchvision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
        "print(f\"torchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZo_wyZRbMmd",
        "outputId": "4f1d5288-ba0e-490d-94ce-ac6d799cbea4"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torchvision version: 0.17.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train our model on the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, made by Zalando Research, that contains grayscale images of 10 different kinds of clothing.\n",
        "\n",
        "To download it, we provide the following parameters:\n",
        "* `root: str` - which folder do you want to download the data to?\n",
        "* `train: Bool` - do you want the training or test split?\n",
        "* `download: Bool` - should the data be downloaded?\n",
        "* `transform: torchvision.transforms` - what transformations would you like to do on the data?\n",
        "* `target_transform` - you can transform the targets (labels) if you like too.\n",
        "\n",
        "Let's load the data using torchvision."
      ],
      "metadata": {
        "id": "VFZFvc4kdVtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train set\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True, # get training data\n",
        "    download=True,\n",
        "    transform=ToTensor(), # images come as PIL format, we want to turn them into Torch tensors\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "# test set\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # get test data\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "-emcG7FnbWjB"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train set\n",
        "train_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True, # get training data\n",
        "    download=True,\n",
        "    transform=ToTensor(), # images come as PIL format, we want to turn them into Torch tensors\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "# test set\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False, # get test data\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pOflXfQDaAX",
        "outputId": "d5e71d5f-7abe-446e-d9c7-869093e2b52f"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 60.000 training images and 10.000 testing images."
      ],
      "metadata": {
        "id": "BJ7J5Z4MVCiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0V1_9EuU_lu",
        "outputId": "7c84a7a5-3e5a-4c19-9c41-8d5eb30a530b"
      },
      "execution_count": 186,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the classes:"
      ],
      "metadata": {
        "id": "Gvq4ZGKCVvco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "print(len(class_names))\n",
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDL13CqaVuXx",
        "outputId": "dd47f5f7-bd2e-4b6a-e637-178892b87a9c"
      },
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['airplane',\n",
              " 'automobile',\n",
              " 'bird',\n",
              " 'cat',\n",
              " 'deer',\n",
              " 'dog',\n",
              " 'frog',\n",
              " 'horse',\n",
              " 'ship',\n",
              " 'truck']"
            ]
          },
          "metadata": {},
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code splits the training data into a training data and validation data."
      ],
      "metadata": {
        "id": "YvApnWeqVVgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data, validation_data = torch.utils.data.random_split(train_data, [50000, 10000])\n",
        "train_data, validation_data = torch.utils.data.random_split(train_data, [40000, 10000])"
      ],
      "metadata": {
        "id": "ZPUueophU1Bc"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the first image (data point) in the train set."
      ],
      "metadata": {
        "id": "_EoNPyLR7Lqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AHLTkPu7TCn",
        "outputId": "d38baa7a-f528-401c-c883-38cca126b310"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.9922, 0.9765, 0.9804,  ..., 0.9216, 0.8745, 0.8471],\n",
              "          [0.9882, 0.9765, 0.9804,  ..., 0.8627, 0.8431, 0.8196],\n",
              "          [0.9922, 0.9843, 0.9922,  ..., 0.9255, 0.9098, 0.8902],\n",
              "          ...,\n",
              "          [0.5765, 0.6078, 0.4863,  ..., 0.2118, 0.2902, 0.2824],\n",
              "          [0.3216, 0.1804, 0.1059,  ..., 0.2118, 0.2510, 0.2471],\n",
              "          [0.0824, 0.0745, 0.0902,  ..., 0.2000, 0.2353, 0.2314]],\n",
              " \n",
              "         [[1.0000, 0.9961, 1.0000,  ..., 0.9804, 0.9843, 0.9804],\n",
              "          [1.0000, 0.9922, 0.9961,  ..., 0.9765, 0.9804, 0.9804],\n",
              "          [1.0000, 0.9961, 0.9961,  ..., 0.9882, 0.9882, 0.9843],\n",
              "          ...,\n",
              "          [0.5765, 0.6000, 0.4745,  ..., 0.2980, 0.3922, 0.3922],\n",
              "          [0.3333, 0.1961, 0.1255,  ..., 0.3176, 0.3725, 0.3725],\n",
              "          [0.0980, 0.0863, 0.0980,  ..., 0.3176, 0.3647, 0.3608]],\n",
              " \n",
              "         [[0.9882, 0.9725, 0.9765,  ..., 0.9725, 0.9647, 0.9725],\n",
              "          [0.9843, 0.9725, 0.9725,  ..., 0.9647, 0.9608, 0.9608],\n",
              "          [0.9882, 0.9765, 0.9765,  ..., 0.9765, 0.9725, 0.9725],\n",
              "          ...,\n",
              "          [0.5451, 0.5451, 0.4353,  ..., 0.2902, 0.4196, 0.4275],\n",
              "          [0.3137, 0.1647, 0.0941,  ..., 0.3020, 0.3961, 0.4039],\n",
              "          [0.0941, 0.0745, 0.0863,  ..., 0.2980, 0.3843, 0.3882]]]),\n",
              " 8)"
            ]
          },
          "metadata": {},
          "execution_count": 189
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each image is a tensor with the following shape."
      ],
      "metadata": {
        "id": "fxAv20e98ZS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08DYvwyA7cVr",
        "outputId": "9ecbf69d-9111-45a8-9147-d352d4bb78a3"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape of the image tensor is `[1, 28, 28]` or more specifically:\n",
        "\n",
        "```\n",
        "[color_channels=1, height=28, width=28]\n",
        "```\n",
        "\n",
        "Having `color_channels=1` means the image is grayscale."
      ],
      "metadata": {
        "id": "flEpM21P8dsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the first image in the train set."
      ],
      "metadata": {
        "id": "XTpz09s6JC0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#image, label = train_data[0]\n",
        "\n",
        "#plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "#plt.title(class_names[label])"
      ],
      "metadata": {
        "id": "wLAkhtiRImaP"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming train_data is your dataset\n",
        "image, label = train_data[1] # Get the first image and its label\n",
        "\n",
        "# Permute the tensor from [C, H, W] to [H, W, C]\n",
        "image = image.permute(1, 2, 0)\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.title(f\"Label: {label}\")\n",
        "plt.axis('off') # Hide the axes\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "6wyzIPCgHvfb",
        "outputId": "6c48427a-19b8-4805-a5dd-7191c1d2acea"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot some more."
      ],
      "metadata": {
        "id": "jLlYNGIZJdgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "fig = plt.figure(figsize=(14, 16))\n",
        "rows, cols = 8, 8\n",
        "for i in range(1, rows * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    img, label = train_data[random_idx]\n",
        "    fig.add_subplot(rows, cols, i)\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    plt.title(class_names[label])\n",
        "    plt.axis(False);\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Kj5IJPx2JgDD",
        "outputId": "8b0f3a9f-5360-4fc1-f039-6afd0220cea1"
      },
      "execution_count": 193,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfig = plt.figure(figsize=(14, 16))\\nrows, cols = 8, 8\\nfor i in range(1, rows * cols + 1):\\n    random_idx = torch.randint(0, len(train_data), size=[1]).item()\\n    img, label = train_data[random_idx]\\n    fig.add_subplot(rows, cols, i)\\n    plt.imshow(img.squeeze(), cmap=\"gray\")\\n    plt.title(class_names[label])\\n    plt.axis(False);\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up till now we have updated the modelparameters once in each epoch. For large dataset this is highly inefficient.\n",
        "\n",
        "It is better to partition the train set into smaller chucks of data, called **batches** (or **minibatches** as this approach works best for very small batch sizes), and update the modelparameters after each batch.  \n",
        "\n",
        "A minibatch size of [32 data points](https://twitter.com/ylecun/status/989610208497360896?s=20&t=N96J_jotN--PYuJk2WcjMw) is a good place to start for most tasks.\n",
        "\n",
        "The PyTorch `DataLoader` class makes using minibatches really easy."
      ],
      "metadata": {
        "id": "xmT4PrIGQAWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(validation_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "_aSQcQfvSG5t"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eFsorRHec00"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Since each image is a 2-dimensional (3 if you count the channel), we first **flatten** the image to just 1 dimension to prepare the input for the input layer of our neural network, i.e. each pixel is one feature."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
        "\n",
        "# Get a single sample\n",
        "x, y = train_data[0]\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x) # perform forward pass\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")\n",
        "\n",
        "# Try uncommenting below and see what happens\n",
        "#print(x)\n",
        "#print(output)"
      ],
      "metadata": {
        "id": "2lk2KEQNYpMx",
        "outputId": "a9113e59-942b-4930-d528-6d00971b3f0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before flattening: torch.Size([3, 32, 32]) -> [color_channels, height, width]\n",
            "Shape after flattening: torch.Size([3, 1024]) -> [color_channels, height*width]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "jhcUJBFuec00"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        num_neurons_layer_2 = 10\n",
        "\n",
        "        self.layer_1 = nn.Linear(in_features=input_dim, out_features=num_neurons_layer_2)\n",
        "        self.layer_2 = nn.Linear(in_features=num_neurons_layer_2, out_features=output_dim)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.layer_1(x))\n",
        "        x = self.layer_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleImageTransformer(nn.Module):\n",
        "    def __init__(self, input_dim=3072, d_model=96, seq_len=32, nhead=8, num_encoder_layers=3, num_classes=10, dropout=0.1):\n",
        "        super(SimpleImageTransformer, self).__init__()\n",
        "\n",
        "        # Ensure the sequence length and d_model are compatible with the input dimensions\n",
        "        assert input_dim % seq_len == 0, \"input_dim must be divisible by seq_len\"\n",
        "        assert d_model == input_dim // seq_len, \"d_model must match the division of input_dim by seq_len\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Correctly reshape x to have d_model as the size of the last dimension\n",
        "        x = x.view(batch_size, self.seq_len, self.d_model)\n",
        "\n",
        "        # Add positional encoding, ensuring dimensions match\n",
        "        x = x + self.positional_encoding[:,:,:x.size(-1)]\n",
        "\n",
        "        # Rearrange to fit Transformer's input requirements\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        # Pass through the Transformer encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # Aggregate the output from the Transformer encoder\n",
        "        x = x.mean(dim=0)\n",
        "\n",
        "        # Pass the aggregated output through the classifier\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "model = SimpleImageTransformer()\n"
      ],
      "metadata": {
        "id": "k8KHQLdTfLMn"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `__init()__`\n",
        "\n",
        "Our neural network has two linear layers. The first layer `layer_1` has `input_dim` (the number of features in our dataset) input features that form the **input layer**. It has `num_neurons_layer_2` output features that form the **hidden layer** where these features are typically called **hidden neurons**.\n",
        "\n",
        "The second layer `layer_2` has `num_neurons_layer_2` input features (neurons) and `output_dim` (which equals to 1 for two-class classification) output features, the **output layer**.\n",
        "\n",
        "An example of this model architecture with `num_neurons_layer_2 = 6` can be seen [here](https://playground.tensorflow.org/#activation=sigmoid&batchSize=30&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=6&seed=0.86658&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false).\n",
        "\n",
        "We will use the Rectified Linear Unit (ReLU) activation function in the hidden layer. In the output layer we use the sigmoid function (through `BCEWithLogitsLoss`, so we not to explicitly apply the sigmoid function during inference (see notebook about logistic regression)).\n",
        "\n",
        "Next, we create an instance of the class `NeuralNetwork`."
      ],
      "metadata": {
        "id": "Fj8ygxXdET-y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "CsEKA3A_ec01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea9e936-ace2-48ba-dda6-11f3ccd9205b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('embedding.weight',\n",
              "              tensor([[-0.0073, -0.0172, -0.0115,  ...,  0.0015,  0.0018, -0.0076],\n",
              "                      [ 0.0159, -0.0128, -0.0158,  ..., -0.0177, -0.0116, -0.0161],\n",
              "                      [ 0.0003,  0.0045,  0.0099,  ..., -0.0049, -0.0166,  0.0176],\n",
              "                      ...,\n",
              "                      [ 0.0009, -0.0092,  0.0052,  ...,  0.0096,  0.0028,  0.0071],\n",
              "                      [-0.0081, -0.0152,  0.0009,  ...,  0.0127,  0.0121,  0.0180],\n",
              "                      [-0.0094,  0.0154, -0.0157,  ...,  0.0063,  0.0180, -0.0061]])),\n",
              "             ('embedding.bias',\n",
              "              tensor([-0.0047,  0.0165, -0.0043, -0.0068, -0.0176, -0.0130, -0.0020,  0.0170,\n",
              "                      -0.0143,  0.0172,  0.0035, -0.0062, -0.0093,  0.0115,  0.0067, -0.0174,\n",
              "                       0.0043, -0.0146,  0.0178, -0.0180,  0.0028,  0.0014,  0.0121,  0.0168,\n",
              "                       0.0149, -0.0098,  0.0156, -0.0063,  0.0073, -0.0042,  0.0034,  0.0157])),\n",
              "             ('transformer_encoder.layers.0.self_attn.in_proj_weight',\n",
              "              tensor([[ 0.0125, -0.0848, -0.0459,  ..., -0.0665, -0.1680,  0.2161],\n",
              "                      [-0.0375, -0.1180,  0.1895,  ...,  0.1126, -0.1981, -0.2038],\n",
              "                      [ 0.0340, -0.0119,  0.0532,  ..., -0.0768,  0.0920, -0.0389],\n",
              "                      ...,\n",
              "                      [ 0.1315, -0.0494, -0.0638,  ...,  0.0264, -0.0806,  0.0357],\n",
              "                      [ 0.1419, -0.1809, -0.0016,  ...,  0.0770, -0.0446, -0.0895],\n",
              "                      [ 0.1465,  0.0753, -0.0490,  ...,  0.1452,  0.0119,  0.0654]])),\n",
              "             ('transformer_encoder.layers.0.self_attn.in_proj_bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.0.self_attn.out_proj.weight',\n",
              "              tensor([[ 0.0280, -0.1120,  0.0769,  ..., -0.0944, -0.0878,  0.1339],\n",
              "                      [-0.1157,  0.1330,  0.0040,  ..., -0.1204, -0.0585,  0.0733],\n",
              "                      [-0.1125,  0.1294, -0.1278,  ..., -0.1436,  0.1681, -0.1551],\n",
              "                      ...,\n",
              "                      [ 0.1107,  0.0346, -0.0788,  ..., -0.1005,  0.1604,  0.0623],\n",
              "                      [-0.1098, -0.1499, -0.0199,  ...,  0.1726, -0.0021,  0.0594],\n",
              "                      [ 0.0032, -0.0397,  0.0072,  ..., -0.1619, -0.1418,  0.0916]])),\n",
              "             ('transformer_encoder.layers.0.self_attn.out_proj.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.0.linear1.weight',\n",
              "              tensor([[-0.0775, -0.1572,  0.0454,  ..., -0.1113,  0.1333, -0.0777],\n",
              "                      [ 0.1620, -0.1224, -0.0232,  ...,  0.0841,  0.0452,  0.0159],\n",
              "                      [-0.0289,  0.0424,  0.0620,  ..., -0.0552, -0.0273, -0.1043],\n",
              "                      ...,\n",
              "                      [ 0.0274, -0.0872, -0.1463,  ...,  0.1302,  0.0390, -0.0128],\n",
              "                      [ 0.0479, -0.1259,  0.1566,  ..., -0.0724, -0.1050, -0.1507],\n",
              "                      [-0.0565, -0.0925, -0.1146,  ...,  0.1695,  0.0186,  0.0798]])),\n",
              "             ('transformer_encoder.layers.0.linear1.bias',\n",
              "              tensor([-0.1214,  0.0816,  0.0184,  ...,  0.1527,  0.1043, -0.1378])),\n",
              "             ('transformer_encoder.layers.0.linear2.weight',\n",
              "              tensor([[-0.0132,  0.0019,  0.0066,  ..., -0.0057, -0.0038,  0.0109],\n",
              "                      [ 0.0010, -0.0073,  0.0082,  ...,  0.0155, -0.0027, -0.0037],\n",
              "                      [-0.0154, -0.0184, -0.0023,  ...,  0.0067, -0.0047, -0.0160],\n",
              "                      ...,\n",
              "                      [ 0.0049, -0.0022,  0.0016,  ...,  0.0180, -0.0067,  0.0020],\n",
              "                      [ 0.0029,  0.0004, -0.0192,  ...,  0.0066,  0.0138,  0.0140],\n",
              "                      [ 0.0007,  0.0203, -0.0133,  ...,  0.0099, -0.0044,  0.0212]])),\n",
              "             ('transformer_encoder.layers.0.linear2.bias',\n",
              "              tensor([ 0.0216,  0.0198, -0.0189,  0.0198, -0.0169,  0.0074,  0.0130, -0.0066,\n",
              "                       0.0079, -0.0107,  0.0118, -0.0159,  0.0213,  0.0122, -0.0163,  0.0032,\n",
              "                       0.0082, -0.0007,  0.0059,  0.0129, -0.0196,  0.0153, -0.0107, -0.0183,\n",
              "                      -0.0215,  0.0207,  0.0047,  0.0156,  0.0202, -0.0126, -0.0060,  0.0096])),\n",
              "             ('transformer_encoder.layers.0.norm1.weight',\n",
              "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "             ('transformer_encoder.layers.0.norm1.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.0.norm2.weight',\n",
              "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "             ('transformer_encoder.layers.0.norm2.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.1.self_attn.in_proj_weight',\n",
              "              tensor([[ 0.0125, -0.0848, -0.0459,  ..., -0.0665, -0.1680,  0.2161],\n",
              "                      [-0.0375, -0.1180,  0.1895,  ...,  0.1126, -0.1981, -0.2038],\n",
              "                      [ 0.0340, -0.0119,  0.0532,  ..., -0.0768,  0.0920, -0.0389],\n",
              "                      ...,\n",
              "                      [ 0.1315, -0.0494, -0.0638,  ...,  0.0264, -0.0806,  0.0357],\n",
              "                      [ 0.1419, -0.1809, -0.0016,  ...,  0.0770, -0.0446, -0.0895],\n",
              "                      [ 0.1465,  0.0753, -0.0490,  ...,  0.1452,  0.0119,  0.0654]])),\n",
              "             ('transformer_encoder.layers.1.self_attn.in_proj_bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.1.self_attn.out_proj.weight',\n",
              "              tensor([[ 0.0280, -0.1120,  0.0769,  ..., -0.0944, -0.0878,  0.1339],\n",
              "                      [-0.1157,  0.1330,  0.0040,  ..., -0.1204, -0.0585,  0.0733],\n",
              "                      [-0.1125,  0.1294, -0.1278,  ..., -0.1436,  0.1681, -0.1551],\n",
              "                      ...,\n",
              "                      [ 0.1107,  0.0346, -0.0788,  ..., -0.1005,  0.1604,  0.0623],\n",
              "                      [-0.1098, -0.1499, -0.0199,  ...,  0.1726, -0.0021,  0.0594],\n",
              "                      [ 0.0032, -0.0397,  0.0072,  ..., -0.1619, -0.1418,  0.0916]])),\n",
              "             ('transformer_encoder.layers.1.self_attn.out_proj.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.1.linear1.weight',\n",
              "              tensor([[-0.0775, -0.1572,  0.0454,  ..., -0.1113,  0.1333, -0.0777],\n",
              "                      [ 0.1620, -0.1224, -0.0232,  ...,  0.0841,  0.0452,  0.0159],\n",
              "                      [-0.0289,  0.0424,  0.0620,  ..., -0.0552, -0.0273, -0.1043],\n",
              "                      ...,\n",
              "                      [ 0.0274, -0.0872, -0.1463,  ...,  0.1302,  0.0390, -0.0128],\n",
              "                      [ 0.0479, -0.1259,  0.1566,  ..., -0.0724, -0.1050, -0.1507],\n",
              "                      [-0.0565, -0.0925, -0.1146,  ...,  0.1695,  0.0186,  0.0798]])),\n",
              "             ('transformer_encoder.layers.1.linear1.bias',\n",
              "              tensor([-0.1214,  0.0816,  0.0184,  ...,  0.1527,  0.1043, -0.1378])),\n",
              "             ('transformer_encoder.layers.1.linear2.weight',\n",
              "              tensor([[-0.0132,  0.0019,  0.0066,  ..., -0.0057, -0.0038,  0.0109],\n",
              "                      [ 0.0010, -0.0073,  0.0082,  ...,  0.0155, -0.0027, -0.0037],\n",
              "                      [-0.0154, -0.0184, -0.0023,  ...,  0.0067, -0.0047, -0.0160],\n",
              "                      ...,\n",
              "                      [ 0.0049, -0.0022,  0.0016,  ...,  0.0180, -0.0067,  0.0020],\n",
              "                      [ 0.0029,  0.0004, -0.0192,  ...,  0.0066,  0.0138,  0.0140],\n",
              "                      [ 0.0007,  0.0203, -0.0133,  ...,  0.0099, -0.0044,  0.0212]])),\n",
              "             ('transformer_encoder.layers.1.linear2.bias',\n",
              "              tensor([ 0.0216,  0.0198, -0.0189,  0.0198, -0.0169,  0.0074,  0.0130, -0.0066,\n",
              "                       0.0079, -0.0107,  0.0118, -0.0159,  0.0213,  0.0122, -0.0163,  0.0032,\n",
              "                       0.0082, -0.0007,  0.0059,  0.0129, -0.0196,  0.0153, -0.0107, -0.0183,\n",
              "                      -0.0215,  0.0207,  0.0047,  0.0156,  0.0202, -0.0126, -0.0060,  0.0096])),\n",
              "             ('transformer_encoder.layers.1.norm1.weight',\n",
              "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "             ('transformer_encoder.layers.1.norm1.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.1.norm2.weight',\n",
              "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "             ('transformer_encoder.layers.1.norm2.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.2.self_attn.in_proj_weight',\n",
              "              tensor([[ 0.0125, -0.0848, -0.0459,  ..., -0.0665, -0.1680,  0.2161],\n",
              "                      [-0.0375, -0.1180,  0.1895,  ...,  0.1126, -0.1981, -0.2038],\n",
              "                      [ 0.0340, -0.0119,  0.0532,  ..., -0.0768,  0.0920, -0.0389],\n",
              "                      ...,\n",
              "                      [ 0.1315, -0.0494, -0.0638,  ...,  0.0264, -0.0806,  0.0357],\n",
              "                      [ 0.1419, -0.1809, -0.0016,  ...,  0.0770, -0.0446, -0.0895],\n",
              "                      [ 0.1465,  0.0753, -0.0490,  ...,  0.1452,  0.0119,  0.0654]])),\n",
              "             ('transformer_encoder.layers.2.self_attn.in_proj_bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.2.self_attn.out_proj.weight',\n",
              "              tensor([[ 0.0280, -0.1120,  0.0769,  ..., -0.0944, -0.0878,  0.1339],\n",
              "                      [-0.1157,  0.1330,  0.0040,  ..., -0.1204, -0.0585,  0.0733],\n",
              "                      [-0.1125,  0.1294, -0.1278,  ..., -0.1436,  0.1681, -0.1551],\n",
              "                      ...,\n",
              "                      [ 0.1107,  0.0346, -0.0788,  ..., -0.1005,  0.1604,  0.0623],\n",
              "                      [-0.1098, -0.1499, -0.0199,  ...,  0.1726, -0.0021,  0.0594],\n",
              "                      [ 0.0032, -0.0397,  0.0072,  ..., -0.1619, -0.1418,  0.0916]])),\n",
              "             ('transformer_encoder.layers.2.self_attn.out_proj.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.2.linear1.weight',\n",
              "              tensor([[-0.0775, -0.1572,  0.0454,  ..., -0.1113,  0.1333, -0.0777],\n",
              "                      [ 0.1620, -0.1224, -0.0232,  ...,  0.0841,  0.0452,  0.0159],\n",
              "                      [-0.0289,  0.0424,  0.0620,  ..., -0.0552, -0.0273, -0.1043],\n",
              "                      ...,\n",
              "                      [ 0.0274, -0.0872, -0.1463,  ...,  0.1302,  0.0390, -0.0128],\n",
              "                      [ 0.0479, -0.1259,  0.1566,  ..., -0.0724, -0.1050, -0.1507],\n",
              "                      [-0.0565, -0.0925, -0.1146,  ...,  0.1695,  0.0186,  0.0798]])),\n",
              "             ('transformer_encoder.layers.2.linear1.bias',\n",
              "              tensor([-0.1214,  0.0816,  0.0184,  ...,  0.1527,  0.1043, -0.1378])),\n",
              "             ('transformer_encoder.layers.2.linear2.weight',\n",
              "              tensor([[-0.0132,  0.0019,  0.0066,  ..., -0.0057, -0.0038,  0.0109],\n",
              "                      [ 0.0010, -0.0073,  0.0082,  ...,  0.0155, -0.0027, -0.0037],\n",
              "                      [-0.0154, -0.0184, -0.0023,  ...,  0.0067, -0.0047, -0.0160],\n",
              "                      ...,\n",
              "                      [ 0.0049, -0.0022,  0.0016,  ...,  0.0180, -0.0067,  0.0020],\n",
              "                      [ 0.0029,  0.0004, -0.0192,  ...,  0.0066,  0.0138,  0.0140],\n",
              "                      [ 0.0007,  0.0203, -0.0133,  ...,  0.0099, -0.0044,  0.0212]])),\n",
              "             ('transformer_encoder.layers.2.linear2.bias',\n",
              "              tensor([ 0.0216,  0.0198, -0.0189,  0.0198, -0.0169,  0.0074,  0.0130, -0.0066,\n",
              "                       0.0079, -0.0107,  0.0118, -0.0159,  0.0213,  0.0122, -0.0163,  0.0032,\n",
              "                       0.0082, -0.0007,  0.0059,  0.0129, -0.0196,  0.0153, -0.0107, -0.0183,\n",
              "                      -0.0215,  0.0207,  0.0047,  0.0156,  0.0202, -0.0126, -0.0060,  0.0096])),\n",
              "             ('transformer_encoder.layers.2.norm1.weight',\n",
              "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "             ('transformer_encoder.layers.2.norm1.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.2.norm2.weight',\n",
              "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "             ('transformer_encoder.layers.2.norm2.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('output_layer.weight',\n",
              "              tensor([[-0.1207, -0.0488,  0.1407,  0.0072, -0.1045,  0.0981,  0.0093, -0.1330,\n",
              "                       -0.1298,  0.1662, -0.0694,  0.0897, -0.0191,  0.1293, -0.0415,  0.0659,\n",
              "                       -0.0386, -0.0828,  0.1650,  0.1618, -0.1559,  0.0957,  0.0163,  0.0236,\n",
              "                        0.1522, -0.1387,  0.1059, -0.0102, -0.1627, -0.1403, -0.1214, -0.1308],\n",
              "                      [-0.0682,  0.0576,  0.0554,  0.0031,  0.0937,  0.1742, -0.0362,  0.0606,\n",
              "                        0.0027, -0.0421, -0.1152, -0.0942,  0.0394, -0.0067, -0.1207,  0.1225,\n",
              "                       -0.0704,  0.0382,  0.0004, -0.0133, -0.1528, -0.1027,  0.0313,  0.1146,\n",
              "                       -0.0802,  0.0455,  0.1388, -0.1251,  0.0416, -0.1560, -0.0145, -0.1152],\n",
              "                      [ 0.0943,  0.0197,  0.0606,  0.0045,  0.0684, -0.0796, -0.0338, -0.0386,\n",
              "                        0.0342, -0.0570, -0.0089, -0.1162, -0.0625,  0.0079,  0.0261, -0.1060,\n",
              "                       -0.1054, -0.0449, -0.1302,  0.1611, -0.1171, -0.0615, -0.0460, -0.1087,\n",
              "                       -0.1009, -0.0868, -0.1636,  0.1155,  0.0898, -0.1244, -0.1147, -0.1240],\n",
              "                      [-0.0255,  0.1697,  0.0467,  0.1086, -0.1722,  0.0761, -0.1729,  0.1512,\n",
              "                       -0.0620,  0.1327, -0.1077,  0.0606,  0.1595,  0.1440,  0.0385,  0.1090,\n",
              "                        0.0983,  0.0550,  0.0439, -0.1137,  0.0363,  0.1039, -0.0804,  0.1725,\n",
              "                        0.1629,  0.0998, -0.0556, -0.1593,  0.0938,  0.0231, -0.1186,  0.0324],\n",
              "                      [-0.1406, -0.0406, -0.1635, -0.1163,  0.0987, -0.1684, -0.0168, -0.1757,\n",
              "                        0.1187,  0.1272, -0.0648, -0.0367, -0.1367,  0.1207, -0.1253,  0.1268,\n",
              "                       -0.1441, -0.0877,  0.0412, -0.0127, -0.1607, -0.1475, -0.0332, -0.1451,\n",
              "                        0.0352, -0.1059,  0.1426, -0.0881,  0.1609, -0.0822,  0.0730, -0.0050],\n",
              "                      [-0.0765, -0.1128,  0.0641, -0.0457,  0.1175, -0.1208,  0.0085, -0.0782,\n",
              "                       -0.0692, -0.0847, -0.1412, -0.0903,  0.1582, -0.1609,  0.1452,  0.1633,\n",
              "                        0.0372, -0.1064, -0.0466,  0.1270,  0.1123, -0.0543,  0.0631, -0.0788,\n",
              "                       -0.0039, -0.1688,  0.0960, -0.1617, -0.1135,  0.0263,  0.0965,  0.1243],\n",
              "                      [-0.1651, -0.1097,  0.0065,  0.1168,  0.0377, -0.0711, -0.1358,  0.0459,\n",
              "                        0.0986, -0.1347, -0.0254, -0.1562, -0.0756,  0.1502,  0.0628, -0.0324,\n",
              "                       -0.0633,  0.0138, -0.0744, -0.0273,  0.0610, -0.0268, -0.0153,  0.1237,\n",
              "                       -0.0587,  0.1274, -0.0211, -0.1764,  0.1195, -0.1004, -0.1508, -0.0833],\n",
              "                      [ 0.0089, -0.1303, -0.1180, -0.0038,  0.0468,  0.1430,  0.1279,  0.0075,\n",
              "                        0.0859, -0.1405, -0.0656,  0.0172, -0.0777, -0.0551,  0.1486, -0.1270,\n",
              "                        0.0518,  0.0678,  0.0346, -0.0644,  0.1544, -0.1529,  0.0373,  0.1582,\n",
              "                       -0.1382, -0.0596,  0.0256, -0.0468,  0.1056,  0.1338, -0.1204,  0.0699],\n",
              "                      [-0.1098, -0.0595, -0.0630, -0.1667, -0.0936,  0.1011,  0.0029,  0.0923,\n",
              "                       -0.0116,  0.1756,  0.0153,  0.1157,  0.0812, -0.1333, -0.1718, -0.0280,\n",
              "                        0.0876,  0.1022, -0.0022, -0.0769, -0.1742, -0.0748,  0.0301, -0.0130,\n",
              "                       -0.0865,  0.0575, -0.0735,  0.0019,  0.0573, -0.1451,  0.1499, -0.0200],\n",
              "                      [ 0.1121, -0.0111, -0.0664,  0.0374,  0.1163,  0.0704, -0.0078, -0.1183,\n",
              "                       -0.0784, -0.1382,  0.0184,  0.0220, -0.0091,  0.1159, -0.1479, -0.1372,\n",
              "                       -0.1140, -0.0181,  0.0302,  0.1672, -0.0085,  0.1482,  0.0026,  0.0016,\n",
              "                        0.1579, -0.0035,  0.1083,  0.0070,  0.1704, -0.1054,  0.1610, -0.1342]])),\n",
              "             ('output_layer.bias',\n",
              "              tensor([-0.1674,  0.0604, -0.0868, -0.0037,  0.1760, -0.0224,  0.0833, -0.0611,\n",
              "                      -0.0985,  0.1766]))])"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ],
      "source": [
        "# Two inputs x_1 and x_2\n",
        "#input_dim = 784\n",
        "input_dim = 32*32*3\n",
        "# Single binary output\n",
        "output_dim = 10\n",
        "\n",
        "# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\n",
        "#model = NeuralNetwork(input_dim, output_dim)\n",
        "model = ImageTransformer(input_dim, output_dim)\n",
        "\n",
        "model.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKdLN7nuheb"
      },
      "source": [
        "### `forward()`\n",
        "\n",
        "The `forward()` method applies the neural network to the provided feature vectors. Here we see that the data is first passed through `layer_1`, then through the ReLU activations that then pass through `layer_2`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(device)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "oVhnxzf1Q1JX",
        "outputId": "6a6db327-15e6-4a98-8943-f4f747d19d50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SimpleImageTransformer(\n",
              "  (encoder_layer): TransformerEncoderLayer(\n",
              "    (self_attn): MultiheadAttention(\n",
              "      (out_proj): NonDynamicallyQuantizableLinear(in_features=96, out_features=96, bias=True)\n",
              "    )\n",
              "    (linear1): Linear(in_features=96, out_features=2048, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (linear2): Linear(in_features=2048, out_features=96, bias=True)\n",
              "    (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "    (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout1): Dropout(p=0.1, inplace=False)\n",
              "    (dropout2): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-2): 3 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=96, out_features=96, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=96, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=96, bias=True)\n",
              "        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=96, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "\n",
        "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)"
      ],
      "metadata": {
        "id": "gsHsfzU9QqyF"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ITlZgU5ec02"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "model.eval()\n",
        "test_acc = 0, 0\n",
        "with torch.inference_mode():\n",
        "    for X, y in test_dataloader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred_logits = model(X)\n",
        "        #y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "        # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "        metric.update(y_pred_logits, y)\n",
        "\"\"\"\n",
        "\n",
        "model.train()\n",
        "\n",
        "test_acc = metric.compute()\n",
        "\n",
        "## Print out what's happening\n",
        "print(f\"\\nTest acc: {test_acc:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD8pnhJUyZUT"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "We use `BCEWithLogitsLoss` as the loss function and SGD, `torch.optim.SGD(params, lr)` as the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "P3T7hpNPec03"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "#the loss function\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "#the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GHdllgi9sfjM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFcKCsPcRfnA"
      },
      "source": [
        "Now we can create and run our training and validation loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1DfhyJ7ec03",
        "outputId": "d4892dfd-0f94-47f9-cddf-15b7deb84ca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 2.15054 | Validation loss: 2.03555, Validation acc: 0.27%\n",
            "Train loss: 1.89993 | Validation loss: 1.81276, Validation acc: 0.33%\n",
            "Train loss: 1.76904 | Validation loss: 1.68996, Validation acc: 0.39%\n",
            "Train loss: 1.67921 | Validation loss: 1.62502, Validation acc: 0.41%\n",
            "Train loss: 1.61110 | Validation loss: 1.61865, Validation acc: 0.41%\n",
            "Train loss: 1.55340 | Validation loss: 1.51208, Validation acc: 0.45%\n",
            "Train loss: 1.49938 | Validation loss: 1.48725, Validation acc: 0.47%\n",
            "Train loss: 1.45345 | Validation loss: 1.44575, Validation acc: 0.48%\n",
            "Train loss: 1.41026 | Validation loss: 1.40280, Validation acc: 0.50%\n",
            "Train loss: 1.36687 | Validation loss: 1.40175, Validation acc: 0.50%\n"
          ]
        }
      ],
      "source": [
        "#number of times we iterate trough the train set\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      #step 1\n",
        "      predictions_train = model(X)\n",
        "\n",
        "      #step 2\n",
        "      loss = loss_func(predictions_train, y)\n",
        "      train_loss += loss\n",
        "\n",
        "      #step 3\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #step 4\n",
        "      loss.backward()\n",
        "\n",
        "      #step 5\n",
        "      optimizer.step()\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.inference_mode():\n",
        "        for X, y in validation_dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            y_pred_logits = model(X)\n",
        "            y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "            # 2. Calculate loss (accumatively)\n",
        "            val_loss += loss_func(y_pred_logits, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            metric.update(y_pred_logits, y)\n",
        "\n",
        "        val_loss /= len(validation_dataloader)\n",
        "\n",
        "    val_acc = metric.compute()\n",
        "    metric.reset()\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"Train loss: {train_loss:.5f} | Validation loss: {val_loss:.5f}, Validation acc: {val_acc:.2f}%\")\n",
        "\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_acc = 0\n",
        "with torch.inference_mode():\n",
        "    for X, y in test_dataloader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred_logits = model(X)\n",
        "        #y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "        # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "        metric.update(y_pred_logits, y)\n",
        "\n",
        "model.train()\n",
        "\n",
        "test_acc = metric.compute()\n",
        "\n",
        "## Print out what's happening\n",
        "print(f\"\\nTest acc: {test_acc:.2f}%\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CS7DuRIwj8m",
        "outputId": "d33c46a0-b2da-487b-d721-184441b973b3"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test acc: 0.50%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture copying TinyVGG from:\n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3, # how big is the square that's going over the image?\n",
        "                      stride=1, # default\n",
        "                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2) # default stride value is same as kernel_size\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # Where did this in_features shape come from?\n",
        "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "            #nn.Linear(in_features=hidden_units*7*7,\n",
        "            #          out_features=output_shape)\n",
        "            nn.Linear(in_features=hidden_units*8*8,\n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.block_2(x)\n",
        "        # print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        # print(x.shape)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kZtwC9wx6t46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)  # Flatten the tensor for the fully connected layer\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = CNN().to(device)"
      ],
      "metadata": {
        "id": "kwPlFU7cc6Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = CNN(input_shape=1,\n",
        "#    hidden_units=10,\n",
        "#    output_shape=len(class_names)).to(device)\n",
        "\n",
        "model = CNN(input_shape=3,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names)).to(device)\n",
        "model"
      ],
      "metadata": {
        "id": "T6w9ATp37VAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "#the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Ee3_pM2A7673"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of times we iterate trough the train set\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      #step 1\n",
        "      predictions_train = model(X)\n",
        "\n",
        "      #step 2\n",
        "      loss = loss_func(predictions_train, y)\n",
        "      train_loss += loss\n",
        "\n",
        "      #step 3\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #step 4\n",
        "      loss.backward()\n",
        "\n",
        "      #step 5\n",
        "      optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.inference_mode():\n",
        "        for X, y in validation_dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            y_pred_logits = model(X)\n",
        "            y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "            # 2. Calculate loss (accumatively)\n",
        "            val_loss += loss_func(y_pred_logits, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            metric.update(y_pred_logits, y)\n",
        "\n",
        "        val_loss /= len(validation_dataloader)\n",
        "\n",
        "    val_acc = metric.compute()\n",
        "\n",
        "    metric.reset()\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"Train loss: {train_loss:.5f} | Validation loss: {val_loss:.5f}, Validation acc: {val_acc:.2f}%\")\n",
        "\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "9h_8lqM77fl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing predictions and evaluating the model\n"
      ],
      "metadata": {
        "id": "e0MESO4WgNQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_acc = 0\n",
        "with torch.inference_mode():\n",
        "    for X, y in test_dataloader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred_logits = model(X)\n",
        "        #y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "        # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "        metric.update(y_pred_logits, y)\n",
        "\n",
        "model.train()\n",
        "\n",
        "test_acc = metric.compute()\n",
        "\n",
        "## Print out what's happening\n",
        "print(f\"\\nTest acc: {test_acc:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "X1saE3rqbrS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkSk6FTvath3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "01_pytorch_workflow.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "3fbe1355223f7b2ffc113ba3ade6a2b520cadace5d5ec3e828c83ce02eb221bf"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}