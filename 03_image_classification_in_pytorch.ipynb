{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdgroeve/D012554_Machine_Learning_2023/blob/main/03_image_classification_in_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "request = requests.get(\"https://raw.githubusercontent.com/sdgroeve/D012554_Machine_Learning_2023/main/utils/utils.py\")\n",
        "with open(\"utils.py\", \"wb\") as f:\n",
        "  f.write(request.content)\n",
        "\n",
        "from utils import plot_decision_boundary"
      ],
      "metadata": {
        "id": "3lBi6JV6xhXZ",
        "cellView": "form"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!pip install tqdm\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU4n2UT6ZsWA",
        "outputId": "b474918f-0f05-4e48-9547-fea9866bb740",
        "collapsed": true,
        "cellView": "form"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.11.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgYkrRCRec0r"
      },
      "source": [
        "# 3. Image Classification in PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZT_ikDC-ec0w",
        "outputId": "01bcf349-ef91-4aec-b2e4-04de57000b7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 265
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "torch.manual_seed(46)\n",
        "\n",
        "# Check PyTorch version\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci_-geIdec0w"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "We will use [`torchvision`](https://pytorch.org/vision/stable/index.html) to load our dataset. This library contains datasets, model architectures and image transformations often used for computer vision prediction tasks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import torchvision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
        "print(f\"torchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZo_wyZRbMmd",
        "outputId": "72121ef7-cfe8-4fd2-d98f-4728ed15cc76"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torchvision version: 0.17.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train our model on the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, made by Zalando Research, that contains grayscale images of 10 different kinds of clothing.\n",
        "\n",
        "To download it, we provide the following parameters:\n",
        "* `root: str` - which folder do you want to download the data to?\n",
        "* `train: Bool` - do you want the training or test split?\n",
        "* `download: Bool` - should the data be downloaded?\n",
        "* `transform: torchvision.transforms` - what transformations would you like to do on the data?\n",
        "* `target_transform` - you can transform the targets (labels) if you like too.\n",
        "\n",
        "Let's load the data using torchvision."
      ],
      "metadata": {
        "id": "VFZFvc4kdVtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train set\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True, # get training data\n",
        "    download=True,\n",
        "    transform=ToTensor(), # images come as PIL format, we want to turn them into Torch tensors\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "# test set\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # get test data\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "-emcG7FnbWjB"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train set\n",
        "train_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True, # get training data\n",
        "    download=True,\n",
        "    transform=ToTensor(), # images come as PIL format, we want to turn them into Torch tensors\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "# test set\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False, # get test data\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3pOflXfQDaAX",
        "outputId": "8fa69010-84b8-42a4-dfd0-313b8c138abb"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 60.000 training images and 10.000 testing images."
      ],
      "metadata": {
        "id": "BJ7J5Z4MVCiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0V1_9EuU_lu",
        "outputId": "4aa24cd6-134a-4993-9eb5-2854b69ff6d4"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the classes:"
      ],
      "metadata": {
        "id": "Gvq4ZGKCVvco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "print(len(class_names))\n",
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDL13CqaVuXx",
        "outputId": "ee41f441-aa99-4873-8867-f255abbb375d"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['airplane',\n",
              " 'automobile',\n",
              " 'bird',\n",
              " 'cat',\n",
              " 'deer',\n",
              " 'dog',\n",
              " 'frog',\n",
              " 'horse',\n",
              " 'ship',\n",
              " 'truck']"
            ]
          },
          "metadata": {},
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code splits the training data into a training data and validation data."
      ],
      "metadata": {
        "id": "YvApnWeqVVgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train_data, validation_data = torch.utils.data.random_split(train_data, [50000, 10000])\n",
        "train_data, validation_data = torch.utils.data.random_split(train_data, [40000, 10000])"
      ],
      "metadata": {
        "id": "ZPUueophU1Bc"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the first image (data point) in the train set."
      ],
      "metadata": {
        "id": "_EoNPyLR7Lqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AHLTkPu7TCn",
        "outputId": "8be91c8b-5826-4d79-df06-45b3de60ef6a"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.9922, 0.9765, 0.9804,  ..., 0.9216, 0.8745, 0.8471],\n",
              "          [0.9882, 0.9765, 0.9804,  ..., 0.8627, 0.8431, 0.8196],\n",
              "          [0.9922, 0.9843, 0.9922,  ..., 0.9255, 0.9098, 0.8902],\n",
              "          ...,\n",
              "          [0.5765, 0.6078, 0.4863,  ..., 0.2118, 0.2902, 0.2824],\n",
              "          [0.3216, 0.1804, 0.1059,  ..., 0.2118, 0.2510, 0.2471],\n",
              "          [0.0824, 0.0745, 0.0902,  ..., 0.2000, 0.2353, 0.2314]],\n",
              " \n",
              "         [[1.0000, 0.9961, 1.0000,  ..., 0.9804, 0.9843, 0.9804],\n",
              "          [1.0000, 0.9922, 0.9961,  ..., 0.9765, 0.9804, 0.9804],\n",
              "          [1.0000, 0.9961, 0.9961,  ..., 0.9882, 0.9882, 0.9843],\n",
              "          ...,\n",
              "          [0.5765, 0.6000, 0.4745,  ..., 0.2980, 0.3922, 0.3922],\n",
              "          [0.3333, 0.1961, 0.1255,  ..., 0.3176, 0.3725, 0.3725],\n",
              "          [0.0980, 0.0863, 0.0980,  ..., 0.3176, 0.3647, 0.3608]],\n",
              " \n",
              "         [[0.9882, 0.9725, 0.9765,  ..., 0.9725, 0.9647, 0.9725],\n",
              "          [0.9843, 0.9725, 0.9725,  ..., 0.9647, 0.9608, 0.9608],\n",
              "          [0.9882, 0.9765, 0.9765,  ..., 0.9765, 0.9725, 0.9725],\n",
              "          ...,\n",
              "          [0.5451, 0.5451, 0.4353,  ..., 0.2902, 0.4196, 0.4275],\n",
              "          [0.3137, 0.1647, 0.0941,  ..., 0.3020, 0.3961, 0.4039],\n",
              "          [0.0941, 0.0745, 0.0863,  ..., 0.2980, 0.3843, 0.3882]]]),\n",
              " 8)"
            ]
          },
          "metadata": {},
          "execution_count": 272
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each image is a tensor with the following shape."
      ],
      "metadata": {
        "id": "fxAv20e98ZS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08DYvwyA7cVr",
        "outputId": "7edc8f22-f430-44f1-dbd0-e4b0d921fa34"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape of the image tensor is `[1, 28, 28]` or more specifically:\n",
        "\n",
        "```\n",
        "[color_channels=1, height=28, width=28]\n",
        "```\n",
        "\n",
        "Having `color_channels=1` means the image is grayscale."
      ],
      "metadata": {
        "id": "flEpM21P8dsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the first image in the train set."
      ],
      "metadata": {
        "id": "XTpz09s6JC0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#image, label = train_data[0]\n",
        "\n",
        "#plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "#plt.title(class_names[label])"
      ],
      "metadata": {
        "id": "wLAkhtiRImaP"
      },
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming train_data is your dataset\n",
        "image, label = train_data[1] # Get the first image and its label\n",
        "\n",
        "# Permute the tensor from [C, H, W] to [H, W, C]\n",
        "image = image.permute(1, 2, 0)\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.title(f\"Label: {label}\")\n",
        "plt.axis('off') # Hide the axes\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "6wyzIPCgHvfb",
        "outputId": "68e0dddb-91ab-44b3-aca2-abc00092aa6f"
      },
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcMUlEQVR4nO3da4ycd5Xn8fNUdd26uu2+2O34Hhw7iZ2JScCTsOAsBjEyCIZxEMsbdpGFJsNwkSIkrlrlwqsoEoEocQSRAIVsdlcaohAhBYLQQDQoMnECJBuT+Ja4E9ux+97VVd1d16f2BezRZB3G56d1Y3v2+5F40zo+/Oupqv5VJXl+TrrdbtcAADCzzIU+AADg4kEoAAAcoQAAcIQCAMARCgAARygAAByhAABwhAIAwBEKAABHKODfpdHRUUuSxL75zW+et51PPfWUJUliTz311HnbCVxsCAVcNB566CFLksSee+65C32UJXH48GH74he/aO9+97utWCxakiQ2Ojp6oY8FvAmhAPyF7N+/3+677z6rVqu2devWC30c4C0RCsBfyEc/+lGbnZ21F1980T75yU9e6OMAb4lQwCWl2Wza7bffbu985ztt+fLlVi6X7aabbrJf/epXf/bPfPvb37aNGzdaqVSy9773vXbw4MGzZg4dOmQf//jHbWhoyIrFou3YscN+8pOfnPM8CwsLdujQIZucnDzn7NDQkPX3959zDriQCAVcUubm5ux73/ue7dq1y+6++2678847bWJiwnbv3m3PP//8WfMPP/yw3Xffffb5z3/evv71r9vBgwft/e9/v42NjfnMH/7wB3vXu95lL7/8sn3ta1+ze+65x8rlsu3Zs8d+/OMf/5vnOXDggG3dutX27dt3vh8qcEH0XOgDAIrBwUEbHR21fD7vP7vlllvs6quvtvvvv9++//3vv2n+2LFjdvToUVu7dq2ZmX3wgx+0G2+80e6++2771re+ZWZmt956q23YsMGeffZZKxQKZmb2uc99znbu3Glf/epX7eabb/4LPTrgwuObAi4p2WzWAyFNU5uenrZ2u207duyw3/3ud2fN79mzxwPBzOyGG26wG2+80X7605+amdn09LT98pe/tE984hNWrVZtcnLSJicnbWpqynbv3m1Hjx61U6dO/dnz7Nq1y7rdrt15553n94ECFwihgEvOD3/4Q9u+fbsVi0UbHh62lStX2hNPPGGVSuWs2S1btpz1syuvvNL/U9Bjx45Zt9u12267zVauXPmm/91xxx1mZjY+Pr6kjwe4mPCPj3BJeeSRR2zv3r22Z88e+/KXv2wjIyOWzWbtrrvusldeeUXel6apmZl96Utfst27d7/lzObNm/+fzgxcSggFXFIeffRR27Rpkz322GOWJIn//P98qv+/HT169KyfHTlyxC6//HIzM9u0aZOZmeVyOfvABz5w/g8MXGL4x0e4pGSzWTMz63a7/rNnnnnG9u/f/5bzjz/++Jv+ncCBAwfsmWeesQ996ENmZjYyMmK7du2yBx980E6fPn3Wn5+YmPg3z6P8J6nApYBvCrjo/OAHP7Ann3zyrJ/feuut9pGPfMQee+wxu/nmm+3DH/6wHT9+3L773e/atm3brFarnfVnNm/ebDt37rTPfvaz1mg07N5777Xh4WH7yle+4jMPPPCA7dy506699lq75ZZbbNOmTTY2Nmb79++3kydP2gsvvPBnz3rgwAF73/veZ3fcccc5/2VzpVKx+++/38zMnn76aTMz27dvnw0MDNjAwIB94QtfiFweYEkRCrjofOc733nLn+/du9f27t1rZ86csQcffNB+/vOf27Zt2+yRRx6xH/3oR29ZVPepT33KMpmM3XvvvTY+Pm433HCD7du3z1avXu0z27Zts+eee86+8Y1v2EMPPWRTU1M2MjJi119/vd1+++3n7XHNzMzYbbfd9qaf3XPPPWZmtnHjRkIBF4Wk+6+/hwMA/r/Gv1MAADhCAQDgCAUAgCMUAACOUAAAOEIBAODC9yk06h1pcbvdDs/+67qCC005y8V07ovJxXNd1HOo88J/zZ1o7x+zNDyZyWjnTtP4udvtlrRbubP7wIu/kHYfe0P7u7sH+9aee+hP/uamj0m7R1bGdz/8Y+3v2nj6tz8Lz16z5Xpp93/9zAPnnOGbAgDAEQoAAEcoAAAcoQAAcIQCAMARCgAARygAAByhAABwhAIAwBEKAABHKAAAXLj7SP1bO5fyb/lcyt1Kb08mc/Fk6lI+P2qX0cXTfaRJkqz4J5Rrrr5m4/PzzTPS5uMnj4Rnc5mytLvVjF/DF156Vtp9pnJQmt+yMR+ebXe1jqfXT54Mz5547XVpd6sd772qtxel3REXz281AMAFRygAAByhAABwhAIAwBEKAABHKAAAHKEAAHCEAgDAEQoAAEcoAABcuOZCpVQdXKo1ChdL3cZS779Un5+LiVqJ0mrFaxd+/exPpN2HX/tNeLZUGJZ2v23D1vBs02al3eOTNWl+w+r4NV+2/DJp9/Fj8ZqL8fFxaXculwvPTsyOSbsj+KYAAHCEAgDAEQoAAEcoAAAcoQAAcIQCAMARCgAARygAAByhAABwhAIAwBEKAAC3ZN1HStfLUnYImak9PEt5lrilvSYaqozOA/HpTNP4H3jt5Ki0+4VDvwvP9mT6pN3T1enwbGUxPmtmls3HO4HMzKwnfg2n505Lq3PFdnh2eb92DecqE+HZ+abWBxXBNwUAgCMUAACOUAAAOEIBAOAIBQCAIxQAAI5QAAA4QgEA4AgFAIAjFAAALlxzMTMzJS0uFgrh2bww+0fx29fVsogkySrT2m5pWjt5aqk0r3weSFLxs8OSftRQruJS93PEn6N22pI2zy/OhGfn5hrS7kp8tbVbVWl3mp4Iz7Y6TWl3Tmy5GJs4Hp79H48+IO3evu268Ow111wj7T7yi5fCs3P1SWl3BN8UAACOUAAAOEIBAOAIBQCAIxQAAI5QAAA4QgEA4AgFAIAjFAAAjlAAADhCAQDgwt1HzcU5aXGzHp/NJFo2KVU8+WJR2t1fHgjPZrNaZ1OzEe96mV+sSLt7B8vSvHXivUBJqj3OVOgc6qRtaXdPNh+eTcTuo9Q60nySxLuP0q7WfXTw8HPh2cmJMWn3YN9weLa6oHUfVRdmw7M58f1TmdSu4cTJ0fDs6tWL0u6+vt7wbCHTJ+0u5+LXpdqqSbsj+KYAAHCEAgDAEQoAAEcoAAAcoQAAcIQCAMARCgAARygAAByhAABwhAIAwIVrLnI9WWlxqx2vL1CrDprN+O3ujWZD2p2249UFy/oHpd2Li/Ph2YMvPS/tHl43pJ2lFu8heftV10u70274ZWVzVa3OI5tVPsek0u6BgRXS/OJi/LVVm5+Rdr9x+o347pq2u9OJV1dks9r7p5XGq1wmprVzlzLrpfnV69fFd5e0OpxuWgrPzgnVH2ZmGeFXbbqgVbOE/v/P+0YAwCWLUAAAOEIBAOAIBQCAIxQAAI5QAAA4QgEA4AgFAIAjFAAAjlAAADhCAQDgwiU1i/V4V46ZWaFQCM8mSSLtzgu7uxbvMjIzazYWw7PTzXjPi5lZsx3vkTk1PirtfmX8RWl+enIqPLusHO8yMjPryfaHZ6tV7XU1Pz8Xnq1Ux6TdV2y6Qpp/4/R4eHZiMt5lZGZWrcXPPl+blXafnjwVnu10tf6ovsFyeLZUyEu7k47Ww1Qsxd/7G982Iu3uFd4TJypav9dMJf7erM3Hf19F8U0BAOAIBQCAIxQAAI5QAAA4QgEA4AgFAIAjFAAAjlAAADhCAQDgCAUAgAvfq33XXXdJi7dv3x6efft1b5d2r7lsTXi2XI7fdm9mNtgfn0+7HWn3TLUanp2qaLUItc60NH/mzInw7OFX49fbzKwxH68AaNbjlSVmZqVS/Pl57cQhaffpsVel+blKLTybzWqfv0rleEXD8MBaaff0xHx4dramVTS0C73h2UZLq4mpzI1K881W/HFmCzPS7qmZ0+HZ2Upb2t1N4tUiw4Mrpd0RfFMAADhCAQDgCAUAgCMUAACOUAAAOEIBAOAIBQCAIxQAAI5QAAA4QgEA4AgFAIBLut1uqGRleVnrqFm+fCA8++lPf1ra3aq3wrML8/F+GjOzv9q+NTy7adMGaXffULwX5snf/FTafXLqmDS/UIt3vey45h3Sbmv3h0c3rvlrafW6NVvCs62O9tznC4k0P1+Ld/eknXifjZnZxPSp8OzM5KS0u1gqhWdrde0a1tuL4dnJ2Qlp99hEvG/IzOzM+Gh4tt6OX28zs/KyeDfVQj0n7U5b8ddKwfqk3T/7by+dc4ZvCgAARygAAByhAABwhAIAwBEKAABHKAAAHKEAAHCEAgDAEQoAAEcoAAAcoQAAcD3RwaQQ7/owM8uV4n0fA0PLpd0v/P6F8Owv/lnrEDpyIt6t0y/0O5mZXbltW3h2MYl3E5mZtZtaR027Fe/tOXT4dWn3f/rbvw/PXn3ljdLuxOIdXN1uR9o9Nz8vzadCt9KpEyek3Q/uezg8W6lOS7v/4TPx56dUGJB2m+XDk9u3bpI2t67U+qMW63Ph2SOvPi3tfunYr8OzaTv+XjMzS9P463amFu+aiuKbAgDAEQoAAEcoAAAcoQAAcIQCAMARCgAARygAAByhAABwhAIAwBEKAAAXrrlYf+WgtLg31xc/hFihUSjHb6XvXdYr7S4NxndPVLR6gaFKvEZhvl6Rds/Xtdvd+/tWhGdvevfN0u7LN+wIzzYa2nNvVg9PLi5q12S2VtXmZ+NVJM//r+ek3Vdsvjw8+/IRreJkbmE2PDt9QnuN12rxs6yvr5d2F4plaX5wYCQ8OzKwWdo92nM4PFuZ1WpiWq12eLbbPf+f6/mmAABwhAIAwBEKAABHKAAAHKEAAHCEAgDAEQoAAEcoAAAcoQAAcIQCAMARCgAAF+4+KvbH+zjMzJb3DoRnS+V+affq1fHOlI9/7L9IuzduWRUfzhal3StGNoZnZ+a0zpl2Z06a7++Pd1ltveqvpd2Ts/EOoSTRPpdkMvH5blfsVcrEe6/MzJqtNL46q+2+7vr4NS+UtU6gXH5ZeDbJaP1R2fBvFLNMpiDtzuWE5WbWFp6ftKNdw75i/HdQsSfe12Vm1l7oxHeX1O6wc+ObAgDAEQoAAEcoAAAcoQAAcIQCAMARCgAARygAAByhAABwhAIAwBEKAACXdINdAJt2aLeB/93f/GN49uYP/2dp98Cy+O3xl60akXZX5mfDs23LSru7SW94dm5BqxfodLT5ZjN+K32nnUi7U4vvTtN4FYGZmXKSrNK5YGaJUKFhZtZuNuJn6cavyR93x6sRutrTYy3hmnfaWr1NJhN/T2R7tN35vPZAC0KdR62qvX8mpl4Pzz7x5P+Udh9/9VR4dtUarSrkif/+m3PO8E0BAOAIBQCAIxQAAI5QAAA4QgEA4AgFAIAjFAAAjlAAADhCAQDgCAUAgCMUAAAuXA7ztx/7j9LijcNXhGd/+/s/SLvT5lR4ttwb7xsyM+sI7TpDKy+TdpcHVoRn61otjLVaTWm+2YzPt8T+m46F6rTMzCxJxOIeYXc3jc+ameV6ctJ8TyZ+9kKP9vlLmU+72u5KbT48m0htU2aFYjE8W1+In8PMrNPW+onK5Xh/VKJ+Pu7Eu+CuuuI6aXW3E+/UyuQq0u7QzvO+EQBwySIUAACOUAAAOEIBAOAIBQCAIxQAAI5QAAA4QgEA4AgFAIAjFAAALlxzsf7y8KiZmQ3m4nlz+MVRafcbJ46EZ9strepg1/t3h2dHVq6Udv/Lb54Jz05VF6Td7VYqzSt1EcVSQTuLcpRE+1ySCPP1erzmwMys0+poZ+nGH2imq+3OZ+P1Et3429jMzDpCLUZGrSER5tVPpLmM9l7OZOLPT66gXcOMcHq1auc977k2PNvqTEu7I/imAABwhAIAwBEKAABHKAAAHKEAAHCEAgDAEQoAAEcoAAAcoQAAcIQCAMARCgAAFy78KPTkpMWzY6fCs/m61juyZcNV4dlm0pR2d9rxvpwzb4xKu984Hu9sOjJ6Utpdb0njNjSwPDx74vhhaffM1Fh4timee3DF6vDs9nfcIO3u6x+Q5lOhzygjfvzK9mTDszmhZ8zMLJEqhLS+oWxW6FUSuqPMzKytvZebzfh8msavt5lZj/A4a02tx+y67Tviw0K/U3jled8IALhkEQoAAEcoAAAcoQAAcIQCAMARCgAARygAAByhAABwhAIAwBEKAABHKAAAXLh0qNvR+olqjenwbLZvUNpd7lsWnk0XtG6QucpMeHZqclzaXS7mw7OXr10r7T4zPSfNnz75enh2/FR81sxsxVB/eLbR1J6fU6OvhGcvWxXvSTIzK1wef37MzNqddng2o1Xr2OJCvBSq0WhouxcX47vr2m6lb6jViJ/DzKxc0H4HFQqF8Ozs7Ky0+7q3Xxue3fK2NdLu4YH14dlOh+4jAMASIhQAAI5QAAA4QgEA4AgFAIAjFAAAjlAAADhCAQDgCAUAgCMUAAAufN/4qlNlafGKtBiezfaVpN2zSSc8u5DVbo1XKgBOnz4t7U6FW9LbzfhjNDOrzExJ86dPnQzPbtm8Wdrdv6wvPNtsabfpV6ovh2dfe/WYtLvb7UrzSr1EtxuvrTAz66Tx+U5He60ojzOXy0m72+149cfM1KS0+6rNm6T5a6/ZGp5dty5eLWFmdvXVV4Zny0Xts3erFX/ukySRdkfwTQEA4AgFAIAjFAAAjlAAADhCAQDgCAUAgCMUAACOUAAAOEIBAOAIBQCAIxQAAC5cDDTct0Na3Mpmw7O50jJpd3NhITz76M/+SdrdqNfDs8PDw9Lu5f3xx1mtVKXdp15/TZpvN+OPc8XwkLT7svUbwrPjYxPS7v6+eAdXoz4v7Z6dHJPmO514z0+zGe/UMjNbrNeEc2jdR0kS/yzY06N1hylnmRX7uvqL8d8pZmarV60Izw4PaL+DJsfivWe1onYNs9mle34i+KYAAHCEAgDAEQoAAEcoAAAcoQAAcIQCAMARCgAARygAAByhAABwhAIAwIXvkS6/4x3S4kYar6Jop1o2XdbohmcHlw9Iux9/6vHwbKFQkHa/5z+8Kzy7Zs16aXe5VJTma7PxioH6olbRkC/FqyjWbeiVdh9+6VB4tjozI+1+47VXpflOGq90aLcb0u602wrPZjKJtLtYLIVnS8u0+odVI/FKlK2b3ybtLvXGz21mlhPqImpzFWn31GT8vd+/LP5+MDPrFR5nmqbS7gi+KQAAHKEAAHCEAgDAEQoAAEcoAAAcoQAAcIQCAMARCgAARygAAByhAABwhAIAwIW7j6qVSWnxwuJseLZWrUu7x87EO216cllp99p1a8Kz1bmqtPvlQ/HentHRE9LumWq8a8rMrJu2w7PVmvY4E4v3sczOav1Ek5Nj4dlWS7smubzWZdVXjs+XerUOoeEVA+HZFStWSLsvW7U6PLtq1Spp98qRleHZvnKftDveePaneaEXqCcX/lVoZmaZTPzzdLZH++zd6cQ7teg+AgAsKUIBAOAIBQCAIxQAAI5QAAA4QgEA4AgFAIAjFAAAjlAAADhCAQDgwvd2N+staXFzMT5bX9R2zy/UwrPZnkTavW5dvAJgekqrRajV4rUL0zPT0u6OfLt7/Fb6V159RdrczcTPsjA/L+1ev2EkPDs0NCTt7u/vl+YHBweFs8RnzcyWL18eni0Wi9LufD4fns3lctLuUqm0ZLt7erQqCmV/NqvV4SjUKopmsxmebbW0350RfFMAADhCAQDgCAUAgCMUAACOUAAAOEIBAOAIBQCAIxQAAI5QAAA4QgEA4AgFAIALl4l0ul1tcybeJZIraN0tAwMD4dl169ZKu83ij3PFihXS5oX5eCFUsxnvJjIzSxKt40nphenp0XphSqV4t8669euk3co1V7t1MhntM5LSl6P0DZlpfUZqb48yv5S7Ve12e8nm1ee+UIj3ni3lc9/paL8nIvimAABwhAIAwBEKAABHKAAAHKEAAHCEAgDAEQoAAEcoAAAcoQAAcIQCAMCFay7kxT3x1b2lkrQ7K9ySrt52r5xbpZwlk1m6c5iZlYrxa14sxW/pNzPL5+P1EmoVhSJN0yXbbWbWFapf1IoGpXZBqUUw0ypR1PqHpaRcbzPtmjebTWn3wsJCeFatuejr61uy3REXzzMOALjgCAUAgCMUAACOUAAAOEIBAOAIBQCAIxQAAI5QAAA4QgEA4AgFAIAjFAAALlywo3YCqZ1DikIh3sWjnqPVaoVn1d4e5Rp2Oh1pd6NRl+aVs6vXUOmoUftslLOovTDq86l0CCmvKzOti6fRaEi7lT4j9blXrolKfa0o8+rvN2W3+tzPzc2FZ5eiq41vCgAARygAAByhAABwhAIAwBEKAABHKAAAHKEAAHCEAgDAEQoAAEcoAAAcoQAAcEvWfdRut8OzS9lpou5WHqfa89LpxK+JmXbu3t6iNK/0R5lpj3NxMd7Fo/TwmC3186P1TSm9QOrjVM6epqm0W5lX3sdm2rnVXqVsVvsdpLxW0lT9HRR/rcj9RMI1rNVq2u4AvikAAByhAABwhAIAwBEKAABHKAAAHKEAAHCEAgDAEQoAAEcoAAAcoQAAcOH7r9Xb3ZvNZnhWraJQbtPvinURyq33ymM0M8vl4re7azUUZsViSZpX6gjU+gfl6LlcTtqtPD/q60p9jTcaS1fnociINSRJNn6WjrZaq9xoaa+rdlOsIRFeW3LlhjDbTrTXVUt4fkz4nRLFNwUAgCMUAACOUAAAOEIBAOAIBQCAIxQAAI5QAAA4QgEA4AgFAIAjFAAAjlAAALhwccZ8bV5a3Gq34ofo0fo7lN6eROyFyefz4Vm1KyfXE9+dzWqdQEmi5btyDbNZsQBHuOZSV45I7RtS55VuJbUnS3l+umKHUK4UL6fqyWuvw0w7fk0y9QVpd7e5KM03usJ1Ed8//cL7M5do17Da2xuebRaK0u4IvikAAByhAABwhAIAwBEKAABHKAAAHKEAAHCEAgDAEQoAAEcoAAAcoQAAcOF+ienpWWlxPh+/lT5TUisd4rNpqlU09GTjVRTF+EM0M636I+1q9Q+djlZ1UCqVwrO5nPb8ZLPZ8KxaFaI+ToVSW2Gm1bOoVS5KLUYn1c6tVGhk2trrsGdReH6qVWl33rRajEwinL2jPc5uIz6fT7UqitLQyvBsu78s7Y7gmwIAwBEKAABHKAAAHKEAAHCEAgDAEQoAAEcoAAAcoQAAcIQCAMARCgAARygAAFy4kKXZ1DpqUqFKpN3W+myWsnMmSeK9PcqsmVnaiffZtFrxWTOzTEabVzqHent7pd1KV1Imo30uSYUXljJrpnU2qfL5eKeWmfa67Yjvn3qzET9HV+sOK2SE90+xT9rdMq2Dq9WNX5dE7I+ybPz9M57Er7eZWdPq4dnW/Pn/XM83BQCAIxQAAI5QAAA4QgEA4AgFAIAjFAAAjlAAADhCAQDgCAUAgCMUAAAufC99oVCSFmeE290b9fht3WZmCwvCbfpidUGSEW7rF++MV65J2m1py0XNZrwWo9PRahT6+/vDs2q1hFKLoZ5bpZyl1Vq651P9ZJcTKjQa6rmF90SpT6u56GSXSfPtVKjmEatCWmn8usxna9Ju68TrPLItrcolgm8KAABHKAAAHKEAAHCEAgDAEQoAAEcoAAAcoQAAcIQCAMARCgAARygAAByhAABwSbfbFRt8AAD/XvFNAQDgCAUAgCMUAACOUAAAOEIBAOAIBQCAIxQAAI5QAAA4QgEA4P43Xo720bxBQ94AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot some more."
      ],
      "metadata": {
        "id": "jLlYNGIZJdgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "fig = plt.figure(figsize=(14, 16))\n",
        "rows, cols = 8, 8\n",
        "for i in range(1, rows * cols + 1):\n",
        "    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n",
        "    img, label = train_data[random_idx]\n",
        "    fig.add_subplot(rows, cols, i)\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "    plt.title(class_names[label])\n",
        "    plt.axis(False);\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Kj5IJPx2JgDD",
        "outputId": "a0885d29-b2c8-43d5-999a-435f6d90d76b"
      },
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfig = plt.figure(figsize=(14, 16))\\nrows, cols = 8, 8\\nfor i in range(1, rows * cols + 1):\\n    random_idx = torch.randint(0, len(train_data), size=[1]).item()\\n    img, label = train_data[random_idx]\\n    fig.add_subplot(rows, cols, i)\\n    plt.imshow(img.squeeze(), cmap=\"gray\")\\n    plt.title(class_names[label])\\n    plt.axis(False);\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up till now we have updated the modelparameters once in each epoch. For large dataset this is highly inefficient.\n",
        "\n",
        "It is better to partition the train set into smaller chucks of data, called **batches** (or **minibatches** as this approach works best for very small batch sizes), and update the modelparameters after each batch.  \n",
        "\n",
        "A minibatch size of [32 data points](https://twitter.com/ylecun/status/989610208497360896?s=20&t=N96J_jotN--PYuJk2WcjMw) is a good place to start for most tasks.\n",
        "\n",
        "The PyTorch `DataLoader` class makes using minibatches really easy."
      ],
      "metadata": {
        "id": "xmT4PrIGQAWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(validation_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "_aSQcQfvSG5t"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eFsorRHec00"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Since each image is a 2-dimensional (3 if you count the channel), we first **flatten** the image to just 1 dimension to prepare the input for the input layer of our neural network, i.e. each pixel is one feature."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
        "\n",
        "# Get a single sample\n",
        "x, y = train_data[0]\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x) # perform forward pass\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")\n",
        "\n",
        "# Try uncommenting below and see what happens\n",
        "#print(x)\n",
        "#print(output)"
      ],
      "metadata": {
        "id": "2lk2KEQNYpMx",
        "outputId": "7643c7de-4be1-4ebb-fded-ba27e0e5c772",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before flattening: torch.Size([3, 32, 32]) -> [color_channels, height, width]\n",
            "Shape after flattening: torch.Size([3, 1024]) -> [color_channels, height*width]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {
        "id": "jhcUJBFuec00"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        num_neurons_layer_2 = 10\n",
        "\n",
        "        self.layer_1 = nn.Linear(in_features=input_dim, out_features=num_neurons_layer_2)\n",
        "        self.layer_2 = nn.Linear(in_features=num_neurons_layer_2, out_features=output_dim)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.layer_1(x))\n",
        "        x = self.layer_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleImageTransformer(nn.Module):\n",
        "    def __init__(self, input_dim=3072, d_model=96, seq_len=32, nhead=8, num_encoder_layers=3, num_classes=10, dropout=0.1):\n",
        "        super(SimpleImageTransformer, self).__init__()\n",
        "\n",
        "        # Ensure the sequence length and d_model are compatible with the input dimensions\n",
        "        assert input_dim % seq_len == 0, \"input_dim must be divisible by seq_len\"\n",
        "        assert d_model == input_dim // seq_len, \"d_model must match the division of input_dim by seq_len\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.seq_len = seq_len\n",
        "\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_len, d_model))\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        self.fc = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Correctly reshape x to have d_model as the size of the last dimension\n",
        "        x = x.view(batch_size, self.seq_len, self.d_model)\n",
        "\n",
        "        # Add positional encoding, ensuring dimensions match\n",
        "        x = x + self.positional_encoding[:,:,:x.size(-1)]\n",
        "\n",
        "        # Rearrange to fit Transformer's input requirements\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        # Pass through the Transformer encoder\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # Aggregate the output from the Transformer encoder\n",
        "        x = x.mean(dim=0)\n",
        "\n",
        "        # Pass the aggregated output through the classifier\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return F.log_softmax(x, dim=-1)\n",
        "\n",
        "model = SimpleImageTransformer()\n"
      ],
      "metadata": {
        "id": "k8KHQLdTfLMn"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `__init()__`\n",
        "\n",
        "Our neural network has two linear layers. The first layer `layer_1` has `input_dim` (the number of features in our dataset) input features that form the **input layer**. It has `num_neurons_layer_2` output features that form the **hidden layer** where these features are typically called **hidden neurons**.\n",
        "\n",
        "The second layer `layer_2` has `num_neurons_layer_2` input features (neurons) and `output_dim` (which equals to 1 for two-class classification) output features, the **output layer**.\n",
        "\n",
        "An example of this model architecture with `num_neurons_layer_2 = 6` can be seen [here](https://playground.tensorflow.org/#activation=sigmoid&batchSize=30&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=6&seed=0.86658&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false).\n",
        "\n",
        "We will use the Rectified Linear Unit (ReLU) activation function in the hidden layer. In the output layer we use the sigmoid function (through `BCEWithLogitsLoss`, so we not to explicitly apply the sigmoid function during inference (see notebook about logistic regression)).\n",
        "\n",
        "Next, we create an instance of the class `NeuralNetwork`."
      ],
      "metadata": {
        "id": "Fj8ygxXdET-y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {
        "id": "CsEKA3A_ec01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e815ba-1188-4c73-9cbe-bdbe14e2b6f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('embedding.weight',\n",
              "              tensor([[ 0.0083,  0.0055,  0.0060,  ..., -0.0160, -0.0063,  0.0137],\n",
              "                      [-0.0040, -0.0043,  0.0070,  ...,  0.0102,  0.0104, -0.0016],\n",
              "                      [-0.0019, -0.0135, -0.0001,  ...,  0.0136,  0.0060, -0.0140],\n",
              "                      ...,\n",
              "                      [ 0.0175, -0.0097,  0.0077,  ...,  0.0052,  0.0033,  0.0177],\n",
              "                      [ 0.0132,  0.0138, -0.0098,  ...,  0.0039, -0.0107,  0.0006],\n",
              "                      [ 0.0158, -0.0167,  0.0046,  ..., -0.0062,  0.0030,  0.0169]])),\n",
              "             ('embedding.bias',\n",
              "              tensor([-0.0131,  0.0012, -0.0054,  0.0088,  0.0080, -0.0107,  0.0141, -0.0058,\n",
              "                       0.0123, -0.0092, -0.0139, -0.0171, -0.0122, -0.0017,  0.0061, -0.0016,\n",
              "                      -0.0073,  0.0108, -0.0047,  0.0103, -0.0097,  0.0098, -0.0102, -0.0120,\n",
              "                      -0.0135,  0.0172, -0.0167,  0.0159,  0.0107, -0.0151, -0.0088, -0.0023])),\n",
              "             ('transformer_encoder.layers.0.self_attn.in_proj_weight',\n",
              "              tensor([[-0.0897, -0.1409, -0.1817,  ..., -0.1314, -0.1557,  0.1713],\n",
              "                      [ 0.2158, -0.1183, -0.2132,  ...,  0.0706,  0.1737,  0.0964],\n",
              "                      [-0.1084,  0.1560,  0.0847,  ..., -0.1411,  0.0758,  0.1865],\n",
              "                      ...,\n",
              "                      [ 0.1875,  0.1933, -0.0769,  ..., -0.0013, -0.0936,  0.1069],\n",
              "                      [ 0.0139, -0.1817, -0.1996,  ..., -0.0923, -0.0613,  0.1142],\n",
              "                      [ 0.0504,  0.1179, -0.1076,  ..., -0.1922, -0.1871,  0.0742]])),\n",
              "             ('transformer_encoder.layers.0.self_attn.in_proj_bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.0.self_attn.out_proj.weight',\n",
              "              tensor([[ 0.0045, -0.0028,  0.0967,  ..., -0.0790,  0.0484, -0.1545],\n",
              "                      [-0.1504,  0.0681, -0.0906,  ..., -0.0763,  0.1033, -0.0552],\n",
              "                      [-0.0091, -0.0548,  0.0505,  ..., -0.0479,  0.0193, -0.0879],\n",
              "                      ...,\n",
              "                      [ 0.1434,  0.0219,  0.0480,  ..., -0.0753, -0.1330, -0.0369],\n",
              "                      [ 0.1652, -0.1670, -0.1279,  ...,  0.1302, -0.1038, -0.0048],\n",
              "                      [-0.0566,  0.1555, -0.1283,  ...,  0.0026,  0.1254,  0.1371]])),\n",
              "             ('transformer_encoder.layers.0.self_attn.out_proj.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.0.linear1.weight',\n",
              "              tensor([[ 0.1686,  0.0777,  0.0712,  ...,  0.0863,  0.0920,  0.1403],\n",
              "                      [ 0.1295,  0.1000,  0.0480,  ...,  0.0885,  0.0981, -0.0487],\n",
              "                      [ 0.0439,  0.0556,  0.1391,  ..., -0.1554, -0.1310, -0.0882],\n",
              "                      ...,\n",
              "                      [-0.1710, -0.0044,  0.0529,  ...,  0.1573, -0.1103,  0.1499],\n",
              "                      [ 0.0703, -0.1756,  0.1056,  ...,  0.0157,  0.1504,  0.0281],\n",
              "                      [ 0.1052, -0.0094,  0.0539,  ...,  0.1023,  0.0875,  0.1329]])),\n",
              "             ('transformer_encoder.layers.0.linear1.bias',\n",
              "              tensor([ 0.0869, -0.0284,  0.0118,  ...,  0.0200,  0.0237,  0.0207])),\n",
              "             ('transformer_encoder.layers.0.linear2.weight',\n",
              "              tensor([[ 0.0120,  0.0160, -0.0156,  ...,  0.0199,  0.0131,  0.0215],\n",
              "                      [ 0.0048, -0.0132,  0.0016,  ..., -0.0052, -0.0132,  0.0190],\n",
              "                      [ 0.0192, -0.0149, -0.0025,  ..., -0.0144,  0.0059, -0.0120],\n",
              "                      ...,\n",
              "                      [-0.0043, -0.0063,  0.0104,  ...,  0.0083,  0.0197, -0.0071],\n",
              "                      [-0.0109,  0.0199, -0.0182,  ..., -0.0013,  0.0090,  0.0115],\n",
              "                      [ 0.0200,  0.0114, -0.0071,  ..., -0.0040, -0.0172,  0.0113]])),\n",
              "             ('transformer_encoder.layers.0.linear2.bias',\n",
              "              tensor([ 0.0003,  0.0136, -0.0078, -0.0042, -0.0199,  0.0219,  0.0059, -0.0155,\n",
              "                       0.0127, -0.0218,  0.0201, -0.0054, -0.0099, -0.0028, -0.0128, -0.0059,\n",
              "                       0.0144,  0.0156, -0.0128,  0.0188,  0.0076,  0.0064,  0.0036,  0.0135,\n",
              "                      -0.0176, -0.0026, -0.0024, -0.0205, -0.0206, -0.0172, -0.0181, -0.0190])),\n",
              "             ('transformer_encoder.layers.0.norm1.weight',\n",
              "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "             ('transformer_encoder.layers.0.norm1.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.0.norm2.weight',\n",
              "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "             ('transformer_encoder.layers.0.norm2.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.1.self_attn.in_proj_weight',\n",
              "              tensor([[-0.0897, -0.1409, -0.1817,  ..., -0.1314, -0.1557,  0.1713],\n",
              "                      [ 0.2158, -0.1183, -0.2132,  ...,  0.0706,  0.1737,  0.0964],\n",
              "                      [-0.1084,  0.1560,  0.0847,  ..., -0.1411,  0.0758,  0.1865],\n",
              "                      ...,\n",
              "                      [ 0.1875,  0.1933, -0.0769,  ..., -0.0013, -0.0936,  0.1069],\n",
              "                      [ 0.0139, -0.1817, -0.1996,  ..., -0.0923, -0.0613,  0.1142],\n",
              "                      [ 0.0504,  0.1179, -0.1076,  ..., -0.1922, -0.1871,  0.0742]])),\n",
              "             ('transformer_encoder.layers.1.self_attn.in_proj_bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.1.self_attn.out_proj.weight',\n",
              "              tensor([[ 0.0045, -0.0028,  0.0967,  ..., -0.0790,  0.0484, -0.1545],\n",
              "                      [-0.1504,  0.0681, -0.0906,  ..., -0.0763,  0.1033, -0.0552],\n",
              "                      [-0.0091, -0.0548,  0.0505,  ..., -0.0479,  0.0193, -0.0879],\n",
              "                      ...,\n",
              "                      [ 0.1434,  0.0219,  0.0480,  ..., -0.0753, -0.1330, -0.0369],\n",
              "                      [ 0.1652, -0.1670, -0.1279,  ...,  0.1302, -0.1038, -0.0048],\n",
              "                      [-0.0566,  0.1555, -0.1283,  ...,  0.0026,  0.1254,  0.1371]])),\n",
              "             ('transformer_encoder.layers.1.self_attn.out_proj.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.1.linear1.weight',\n",
              "              tensor([[ 0.1686,  0.0777,  0.0712,  ...,  0.0863,  0.0920,  0.1403],\n",
              "                      [ 0.1295,  0.1000,  0.0480,  ...,  0.0885,  0.0981, -0.0487],\n",
              "                      [ 0.0439,  0.0556,  0.1391,  ..., -0.1554, -0.1310, -0.0882],\n",
              "                      ...,\n",
              "                      [-0.1710, -0.0044,  0.0529,  ...,  0.1573, -0.1103,  0.1499],\n",
              "                      [ 0.0703, -0.1756,  0.1056,  ...,  0.0157,  0.1504,  0.0281],\n",
              "                      [ 0.1052, -0.0094,  0.0539,  ...,  0.1023,  0.0875,  0.1329]])),\n",
              "             ('transformer_encoder.layers.1.linear1.bias',\n",
              "              tensor([ 0.0869, -0.0284,  0.0118,  ...,  0.0200,  0.0237,  0.0207])),\n",
              "             ('transformer_encoder.layers.1.linear2.weight',\n",
              "              tensor([[ 0.0120,  0.0160, -0.0156,  ...,  0.0199,  0.0131,  0.0215],\n",
              "                      [ 0.0048, -0.0132,  0.0016,  ..., -0.0052, -0.0132,  0.0190],\n",
              "                      [ 0.0192, -0.0149, -0.0025,  ..., -0.0144,  0.0059, -0.0120],\n",
              "                      ...,\n",
              "                      [-0.0043, -0.0063,  0.0104,  ...,  0.0083,  0.0197, -0.0071],\n",
              "                      [-0.0109,  0.0199, -0.0182,  ..., -0.0013,  0.0090,  0.0115],\n",
              "                      [ 0.0200,  0.0114, -0.0071,  ..., -0.0040, -0.0172,  0.0113]])),\n",
              "             ('transformer_encoder.layers.1.linear2.bias',\n",
              "              tensor([ 0.0003,  0.0136, -0.0078, -0.0042, -0.0199,  0.0219,  0.0059, -0.0155,\n",
              "                       0.0127, -0.0218,  0.0201, -0.0054, -0.0099, -0.0028, -0.0128, -0.0059,\n",
              "                       0.0144,  0.0156, -0.0128,  0.0188,  0.0076,  0.0064,  0.0036,  0.0135,\n",
              "                      -0.0176, -0.0026, -0.0024, -0.0205, -0.0206, -0.0172, -0.0181, -0.0190])),\n",
              "             ('transformer_encoder.layers.1.norm1.weight',\n",
              "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "             ('transformer_encoder.layers.1.norm1.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.1.norm2.weight',\n",
              "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "             ('transformer_encoder.layers.1.norm2.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.2.self_attn.in_proj_weight',\n",
              "              tensor([[-0.0897, -0.1409, -0.1817,  ..., -0.1314, -0.1557,  0.1713],\n",
              "                      [ 0.2158, -0.1183, -0.2132,  ...,  0.0706,  0.1737,  0.0964],\n",
              "                      [-0.1084,  0.1560,  0.0847,  ..., -0.1411,  0.0758,  0.1865],\n",
              "                      ...,\n",
              "                      [ 0.1875,  0.1933, -0.0769,  ..., -0.0013, -0.0936,  0.1069],\n",
              "                      [ 0.0139, -0.1817, -0.1996,  ..., -0.0923, -0.0613,  0.1142],\n",
              "                      [ 0.0504,  0.1179, -0.1076,  ..., -0.1922, -0.1871,  0.0742]])),\n",
              "             ('transformer_encoder.layers.2.self_attn.in_proj_bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.2.self_attn.out_proj.weight',\n",
              "              tensor([[ 0.0045, -0.0028,  0.0967,  ..., -0.0790,  0.0484, -0.1545],\n",
              "                      [-0.1504,  0.0681, -0.0906,  ..., -0.0763,  0.1033, -0.0552],\n",
              "                      [-0.0091, -0.0548,  0.0505,  ..., -0.0479,  0.0193, -0.0879],\n",
              "                      ...,\n",
              "                      [ 0.1434,  0.0219,  0.0480,  ..., -0.0753, -0.1330, -0.0369],\n",
              "                      [ 0.1652, -0.1670, -0.1279,  ...,  0.1302, -0.1038, -0.0048],\n",
              "                      [-0.0566,  0.1555, -0.1283,  ...,  0.0026,  0.1254,  0.1371]])),\n",
              "             ('transformer_encoder.layers.2.self_attn.out_proj.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.2.linear1.weight',\n",
              "              tensor([[ 0.1686,  0.0777,  0.0712,  ...,  0.0863,  0.0920,  0.1403],\n",
              "                      [ 0.1295,  0.1000,  0.0480,  ...,  0.0885,  0.0981, -0.0487],\n",
              "                      [ 0.0439,  0.0556,  0.1391,  ..., -0.1554, -0.1310, -0.0882],\n",
              "                      ...,\n",
              "                      [-0.1710, -0.0044,  0.0529,  ...,  0.1573, -0.1103,  0.1499],\n",
              "                      [ 0.0703, -0.1756,  0.1056,  ...,  0.0157,  0.1504,  0.0281],\n",
              "                      [ 0.1052, -0.0094,  0.0539,  ...,  0.1023,  0.0875,  0.1329]])),\n",
              "             ('transformer_encoder.layers.2.linear1.bias',\n",
              "              tensor([ 0.0869, -0.0284,  0.0118,  ...,  0.0200,  0.0237,  0.0207])),\n",
              "             ('transformer_encoder.layers.2.linear2.weight',\n",
              "              tensor([[ 0.0120,  0.0160, -0.0156,  ...,  0.0199,  0.0131,  0.0215],\n",
              "                      [ 0.0048, -0.0132,  0.0016,  ..., -0.0052, -0.0132,  0.0190],\n",
              "                      [ 0.0192, -0.0149, -0.0025,  ..., -0.0144,  0.0059, -0.0120],\n",
              "                      ...,\n",
              "                      [-0.0043, -0.0063,  0.0104,  ...,  0.0083,  0.0197, -0.0071],\n",
              "                      [-0.0109,  0.0199, -0.0182,  ..., -0.0013,  0.0090,  0.0115],\n",
              "                      [ 0.0200,  0.0114, -0.0071,  ..., -0.0040, -0.0172,  0.0113]])),\n",
              "             ('transformer_encoder.layers.2.linear2.bias',\n",
              "              tensor([ 0.0003,  0.0136, -0.0078, -0.0042, -0.0199,  0.0219,  0.0059, -0.0155,\n",
              "                       0.0127, -0.0218,  0.0201, -0.0054, -0.0099, -0.0028, -0.0128, -0.0059,\n",
              "                       0.0144,  0.0156, -0.0128,  0.0188,  0.0076,  0.0064,  0.0036,  0.0135,\n",
              "                      -0.0176, -0.0026, -0.0024, -0.0205, -0.0206, -0.0172, -0.0181, -0.0190])),\n",
              "             ('transformer_encoder.layers.2.norm1.weight',\n",
              "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "             ('transformer_encoder.layers.2.norm1.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('transformer_encoder.layers.2.norm2.weight',\n",
              "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
              "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
              "             ('transformer_encoder.layers.2.norm2.bias',\n",
              "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "                      0., 0., 0., 0., 0., 0., 0., 0.])),\n",
              "             ('output_layer.weight',\n",
              "              tensor([[-0.1572,  0.1067, -0.0235,  0.0535,  0.0300, -0.1274, -0.0197,  0.1432,\n",
              "                        0.0476,  0.0060,  0.0657,  0.0358, -0.1009, -0.0343, -0.1023, -0.1445,\n",
              "                       -0.1211,  0.0765, -0.1022, -0.1203,  0.1691,  0.0698, -0.1755, -0.1573,\n",
              "                        0.1058,  0.0240,  0.0451, -0.0906, -0.1333,  0.0482,  0.1765, -0.1121],\n",
              "                      [ 0.1017,  0.0410, -0.0466, -0.0794,  0.1434,  0.1440,  0.0840,  0.0359,\n",
              "                       -0.1080, -0.1262,  0.0519,  0.1723, -0.0475, -0.0686, -0.0143,  0.0666,\n",
              "                        0.0993, -0.1048, -0.1010,  0.0102, -0.0761,  0.0836,  0.1605, -0.0860,\n",
              "                        0.0864, -0.0401, -0.0723, -0.1300,  0.0307, -0.0787,  0.1702, -0.0929],\n",
              "                      [-0.0437, -0.0128, -0.0714,  0.0407, -0.0601,  0.0766,  0.0631, -0.1727,\n",
              "                       -0.1477, -0.0536, -0.0448,  0.0165,  0.1693, -0.1319,  0.0531,  0.0371,\n",
              "                        0.0202,  0.1223, -0.1028, -0.1209,  0.1318,  0.0719,  0.1050, -0.0670,\n",
              "                       -0.0166,  0.0664,  0.1099, -0.0176, -0.0534,  0.1024,  0.1361, -0.0349],\n",
              "                      [ 0.0789, -0.1458,  0.0683,  0.0344, -0.1115,  0.0324, -0.1519, -0.0337,\n",
              "                       -0.1506,  0.1105,  0.1129, -0.1036,  0.0307,  0.1248, -0.0093, -0.0324,\n",
              "                       -0.1441, -0.0600,  0.1573, -0.0409, -0.0594, -0.1578,  0.1223,  0.0346,\n",
              "                       -0.1711,  0.0806, -0.1740,  0.1582,  0.0484, -0.0339,  0.1226,  0.1221],\n",
              "                      [-0.0298, -0.1588,  0.0673, -0.0920, -0.1255,  0.0050, -0.0505, -0.0790,\n",
              "                        0.1224,  0.1273,  0.0814,  0.0925,  0.0098,  0.0293,  0.1325, -0.0492,\n",
              "                       -0.1111, -0.0372, -0.1348, -0.0465,  0.0550, -0.0359, -0.1161, -0.0603,\n",
              "                        0.1048,  0.0654,  0.1343, -0.0477,  0.0146, -0.0573, -0.1746, -0.0527],\n",
              "                      [-0.1381, -0.1737, -0.0725,  0.0969, -0.1644,  0.1177,  0.1564, -0.1552,\n",
              "                       -0.0613,  0.1398,  0.1661,  0.0206,  0.1546, -0.0195, -0.1132,  0.1285,\n",
              "                        0.0944, -0.0030, -0.1324, -0.0828, -0.1578, -0.0870, -0.0731,  0.0223,\n",
              "                        0.1257, -0.0655,  0.0141, -0.1173, -0.0339, -0.0096, -0.0559, -0.1003],\n",
              "                      [ 0.0840,  0.1505,  0.0817, -0.0909, -0.1399,  0.1532, -0.0191, -0.1496,\n",
              "                        0.1226,  0.0077,  0.1251,  0.1311, -0.1363,  0.0262,  0.0133, -0.0211,\n",
              "                       -0.1616,  0.0341, -0.0630, -0.1405,  0.1398,  0.0882, -0.0791,  0.0536,\n",
              "                       -0.0350,  0.0245, -0.1293,  0.1348,  0.1493,  0.0577,  0.0454,  0.1680],\n",
              "                      [-0.0157,  0.1682, -0.0810, -0.1210,  0.1063,  0.1685, -0.1595, -0.0282,\n",
              "                       -0.0149, -0.1573, -0.0026,  0.1230, -0.1715,  0.1656, -0.1624,  0.0624,\n",
              "                        0.1765,  0.0789,  0.0920,  0.1735, -0.0853,  0.0895,  0.0131, -0.0208,\n",
              "                       -0.0981,  0.0547,  0.0467,  0.1355, -0.0188,  0.0064, -0.1132, -0.0768],\n",
              "                      [ 0.0268, -0.0116,  0.1244, -0.0699,  0.0152, -0.0268,  0.0538, -0.0232,\n",
              "                       -0.1455,  0.0348,  0.1551,  0.1472,  0.0081,  0.0607, -0.0708, -0.1571,\n",
              "                       -0.1379,  0.1398, -0.1203,  0.0295, -0.0557, -0.1078, -0.0273,  0.0237,\n",
              "                        0.1207, -0.1675, -0.0243, -0.1764, -0.0313, -0.0410,  0.0265, -0.0252],\n",
              "                      [-0.0905, -0.0238,  0.0819,  0.0769, -0.1263, -0.0628, -0.1323,  0.1256,\n",
              "                        0.0845,  0.0093, -0.1072,  0.1016,  0.1145, -0.1089,  0.1220,  0.0354,\n",
              "                       -0.0810, -0.1513,  0.0227, -0.0800, -0.0741, -0.0480,  0.0955, -0.0117,\n",
              "                        0.1344,  0.0560,  0.0852, -0.0805, -0.1133,  0.0111,  0.1097,  0.0597]])),\n",
              "             ('output_layer.bias',\n",
              "              tensor([-0.0474, -0.1328, -0.1420, -0.0172,  0.0619,  0.1740, -0.0828,  0.1185,\n",
              "                       0.1701, -0.1098]))])"
            ]
          },
          "metadata": {},
          "execution_count": 281
        }
      ],
      "source": [
        "# Two inputs x_1 and x_2\n",
        "#input_dim = 784\n",
        "input_dim = 32*32*3\n",
        "# Single binary output\n",
        "output_dim = 10\n",
        "\n",
        "# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\n",
        "#model = NeuralNetwork(input_dim, output_dim)\n",
        "\n",
        "model.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKdLN7nuheb"
      },
      "source": [
        "### `forward()`\n",
        "\n",
        "The `forward()` method applies the neural network to the provided feature vectors. Here we see that the data is first passed through `layer_1`, then through the ReLU activations that then pass through `layer_2`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(device)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "oVhnxzf1Q1JX",
        "outputId": "36498ce6-b23d-485f-93bd-35d76e9c5fb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageTransformer(\n",
              "  (embedding): Linear(in_features=3072, out_features=32, bias=True)\n",
              "  (transformer_encoder): TransformerEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-2): 3 x TransformerEncoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=32, out_features=2048, bias=True)\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (linear2): Linear(in_features=2048, out_features=32, bias=True)\n",
              "        (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.1, inplace=False)\n",
              "        (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (output_layer): Linear(in_features=32, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "\n",
        "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)"
      ],
      "metadata": {
        "id": "gsHsfzU9QqyF"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {
        "id": "-ITlZgU5ec02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "286ca1f7-db0c-417d-9dc2-7c577be6b05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test acc: 0.00%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "model.eval()\n",
        "test_acc = 0, 0\n",
        "with torch.inference_mode():\n",
        "    for X, y in test_dataloader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred_logits = model(X)\n",
        "        #y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "        # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "        metric.update(y_pred_logits, y)\n",
        "\"\"\"\n",
        "\n",
        "model.train()\n",
        "\n",
        "test_acc = metric.compute()\n",
        "\n",
        "## Print out what's happening\n",
        "print(f\"\\nTest acc: {test_acc:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD8pnhJUyZUT"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "We use `BCEWithLogitsLoss` as the loss function and SGD, `torch.optim.SGD(params, lr)` as the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "P3T7hpNPec03"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "#the loss function\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "#the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GHdllgi9sfjM"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFcKCsPcRfnA"
      },
      "source": [
        "Now we can create and run our training and validation loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "k1DfhyJ7ec03",
        "outputId": "0732a58c-8f1c-46b4-bfd2-807952d670f5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (3072x32 and 3072x32)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-286-e5156bcfa1cd>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0;31m#step 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0mpredictions_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0;31m#step 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-206-a8814705b0b9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Input shape is (batch_size, input_dim)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Convert input to (batch_size, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Now shape is (batch_size, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Reshape & transpose to fit the transformer input requirements: (seq_len, batch_size, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3072x32 and 3072x32)"
          ]
        }
      ],
      "source": [
        "#number of times we iterate trough the train set\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      #step 1\n",
        "      predictions_train = model(X)\n",
        "\n",
        "      #step 2\n",
        "      loss = loss_func(predictions_train, y)\n",
        "      train_loss += loss\n",
        "\n",
        "      #step 3\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #step 4\n",
        "      loss.backward()\n",
        "\n",
        "      #step 5\n",
        "      optimizer.step()\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.inference_mode():\n",
        "        for X, y in validation_dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            y_pred_logits = model(X)\n",
        "            y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "            # 2. Calculate loss (accumatively)\n",
        "            val_loss += loss_func(y_pred_logits, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            metric.update(y_pred_logits, y)\n",
        "\n",
        "        val_loss /= len(validation_dataloader)\n",
        "\n",
        "    val_acc = metric.compute()\n",
        "    metric.reset()\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"Train loss: {train_loss:.5f} | Validation loss: {val_loss:.5f}, Validation acc: {val_acc:.2f}%\")\n",
        "\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_acc = 0\n",
        "with torch.inference_mode():\n",
        "    for X, y in test_dataloader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred_logits = model(X)\n",
        "        #y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "        # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "        metric.update(y_pred_logits, y)\n",
        "\n",
        "model.train()\n",
        "\n",
        "test_acc = metric.compute()\n",
        "\n",
        "## Print out what's happening\n",
        "print(f\"\\nTest acc: {test_acc:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "8CS7DuRIwj8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture copying TinyVGG from:\n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3, # how big is the square that's going over the image?\n",
        "                      stride=1, # default\n",
        "                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2) # default stride value is same as kernel_size\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # Where did this in_features shape come from?\n",
        "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "            #nn.Linear(in_features=hidden_units*7*7,\n",
        "            #          out_features=output_shape)\n",
        "            nn.Linear(in_features=hidden_units*8*8,\n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.block_2(x)\n",
        "        # print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        # print(x.shape)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kZtwC9wx6t46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 4 * 4)  # Flatten the tensor for the fully connected layer\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = CNN().to(device)"
      ],
      "metadata": {
        "id": "kwPlFU7cc6Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models, transforms, datasets\n",
        "\n",
        "# 1. Load the pre-trained model\n",
        "#model = models.resnet18(pretrained=True)\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# 2. Modify the model for binary classification\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "6g4bEUSGpkhe",
        "outputId": "5cf3ffa3-585a-4917-b4c6-81bfe4b3d6c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|| 97.8M/97.8M [00:00<00:00, 135MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-97f3a4ee77cd>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 2. Modify the model for binary classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnum_ftrs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_ftrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model = CNN(input_shape=1,\n",
        "#    hidden_units=10,\n",
        "#    output_shape=len(class_names)).to(device)\n",
        "\n",
        "model = CNN(input_shape=3,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names)).to(device)\n",
        "model"
      ],
      "metadata": {
        "id": "T6w9ATp37VAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "#the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Ee3_pM2A7673"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of times we iterate trough the train set\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      #step 1\n",
        "      predictions_train = model(X)\n",
        "\n",
        "      #step 2\n",
        "      loss = loss_func(predictions_train, y)\n",
        "      train_loss += loss\n",
        "\n",
        "      #step 3\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #step 4\n",
        "      loss.backward()\n",
        "\n",
        "      #step 5\n",
        "      optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.inference_mode():\n",
        "        for X, y in validation_dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            y_pred_logits = model(X)\n",
        "            y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "            # 2. Calculate loss (accumatively)\n",
        "            val_loss += loss_func(y_pred_logits, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            metric.update(y_pred_logits, y)\n",
        "\n",
        "        val_loss /= len(validation_dataloader)\n",
        "\n",
        "    val_acc = metric.compute()\n",
        "\n",
        "    metric.reset()\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"Train loss: {train_loss:.5f} | Validation loss: {val_loss:.5f}, Validation acc: {val_acc:.2f}%\")\n",
        "\n",
        "    model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h_8lqM77fl-",
        "outputId": "9ca6ef23-0ce3-4317-f63d-3ce62a294d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 3082.56616 | Validation loss: 1.81202, Validation acc: 0.40%\n",
            "Train loss: 1756.05103 | Validation loss: 1.23836, Validation acc: 0.55%\n",
            "Train loss: 1425.01526 | Validation loss: 1.29142, Validation acc: 0.57%\n",
            "Train loss: 1290.41431 | Validation loss: 4.45683, Validation acc: 0.41%\n",
            "Train loss: 1158.48730 | Validation loss: 0.89761, Validation acc: 0.69%\n",
            "Train loss: 1052.65710 | Validation loss: 0.82221, Validation acc: 0.72%\n",
            "Train loss: 966.91443 | Validation loss: 1.31810, Validation acc: 0.58%\n",
            "Train loss: 824.26489 | Validation loss: 1.03970, Validation acc: 0.72%\n",
            "Train loss: 709.62427 | Validation loss: 0.83558, Validation acc: 0.73%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing predictions and evaluating the model\n"
      ],
      "metadata": {
        "id": "e0MESO4WgNQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_acc = 0\n",
        "with torch.inference_mode():\n",
        "    for X, y in test_dataloader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred_logits = model(X)\n",
        "        #y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "        # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "        metric.update(y_pred_logits, y)\n",
        "\n",
        "model.train()\n",
        "\n",
        "test_acc = metric.compute()\n",
        "\n",
        "## Print out what's happening\n",
        "print(f\"\\nTest acc: {test_acc:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "X1saE3rqbrS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkSk6FTvath3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "01_pytorch_workflow.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "3fbe1355223f7b2ffc113ba3ade6a2b520cadace5d5ec3e828c83ce02eb221bf"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}