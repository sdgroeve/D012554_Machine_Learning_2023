{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdgroeve/D012554_Machine_Learning_2023/blob/main/01_logisitc_regression_in_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import requests\n",
        "from pathlib import Path \n",
        "\n",
        "request = requests.get(\"https://raw.githubusercontent.com/sdgroeve/D012554_Machine_Learning_2023/main/utils/utils.py\")\n",
        "with open(\"utils.py\", \"wb\") as f:\n",
        "  f.write(request.content)\n",
        "\n",
        "from utils import plot_decision_boundary"
      ],
      "metadata": {
        "cellView": "form",
        "id": "T11x0t4YnjOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "hU4n2UT6ZsWA",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgYkrRCRec0r"
      },
      "source": [
        "# 1. Logitic regression in PyTorch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9EOt5cbod6l"
      },
      "source": [
        "[PyTorch](https://pytorch.org/) is an open source deep learning framework. \n",
        "\n",
        "In this notebook we will learn about a PyTorch training and evaluation workflow for fitting a logistic regression model on a toy dataset.\n",
        "\n",
        "First, we import the required PyTorch libraries and fix the random seed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT_ikDC-ec0w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn \n",
        "\n",
        "torch.manual_seed(46)\n",
        "\n",
        "# Check PyTorch version\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51Ug7Ug123Ip"
      },
      "source": [
        "A tyical PyTorch workflow involves:\n",
        "\n",
        "- Preparing the data\n",
        "- Building the model\n",
        "- Fitting the model to the data (training)\n",
        "- Computing predictions and evaluating the model\n",
        "- Saving the model\n",
        "\n",
        "Let's discuss these steps in more detail by fitting a logistic regression model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci_-geIdec0w"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "The dataset is in a flat file called `dataset_logistic_regression.csv`. \n",
        "\n",
        "We read this file into a Pandas DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_path = \"https://raw.githubusercontent.com/sdgroeve/D012554_Machine_Learning_2023/main/datasets/dataset_logistic_regression.csv\"\n",
        "\n",
        "dataset = pd.read_csv(data_path)\n",
        "\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "GZo_wyZRbMmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset as two features `x_1` and `x_2`, and one label `y`. \n",
        "\n",
        "Let's plot this data. "
      ],
      "metadata": {
        "id": "VFZFvc4kdVtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.lmplot(x=\"x_1\",y=\"x_2\",hue=\"y\",data=dataset,fit_reg=False, palette=\"Set1\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-emcG7FnbWjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We put the feature columns in a DataFrame called `X` and the label column in a DataFrame (or Series) called `y`."
      ],
      "metadata": {
        "id": "G1s6QsJwd4md"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = dataset.pop('y')\n",
        "X = dataset"
      ],
      "metadata": {
        "id": "Nrpm82fQeDK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A typical deep learning workflow would involve a train, a validation and a test split of the dataset. \n",
        "\n",
        "The train set is used to fit the modelparameters. \n",
        "\n",
        "The validation set is used to evaluate different hyperparameter values. \n",
        "\n",
        "The test set is used to estimate the exppected prediction error on unseen external data. The data in the test set should not be used of training the network, nor for evaluating different hyperparameter values."
      ],
      "metadata": {
        "id": "EoL6jszWeRBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1)\n",
        "print(X_train.shape)\n",
        "print(X_val.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "jj0jyNo1W_Cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch we work with tensor representaions of the dataset. A PyTorch Tensor is basically the same as a numpy array, it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array optimized for arbitrary numeric computation.\n",
        "\n",
        "To create a tensor we need to first extract the numpy data from the Pandas DataFrames."
      ],
      "metadata": {
        "id": "n2Dih7Y91UiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train)"
      ],
      "metadata": {
        "id": "1O_-6hSWvNMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test = X_train.values, X_val.values, X_test.values\n",
        "y_train, y_val, y_test = y_train.values, y_val.values, y_test.values"
      ],
      "metadata": {
        "id": "5vbNv-mhfrg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train)"
      ],
      "metadata": {
        "id": "uXsT48Vjf3Z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create the Tensors."
      ],
      "metadata": {
        "id": "2G5dD7MC2GNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, X_test = torch.Tensor(X_train),torch.Tensor(X_val),torch.Tensor(X_test)\n",
        "y_train, y_val, y_test = torch.Tensor(y_train),torch.Tensor(y_val),torch.Tensor(y_test)"
      ],
      "metadata": {
        "id": "m9vgmY8UWPka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_train)"
      ],
      "metadata": {
        "id": "4iPlRBzKvaXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eFsorRHec00"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "To build a model in PyTorch we need to create a subclass of `torch.nn.Module` such that this subclass inherits all functionality required for fitting our model.\n",
        "\n",
        "The following code builds a logistic regression model as a class called `LogisticRegression`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhcUJBFuec00"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "\n",
        "    self.linear = torch.nn.Linear(input_dim, output_dim)   \n",
        "    \n",
        "    torch.nn.init.normal_(self.linear.weight) \n",
        "    torch.nn.init.normal_(self.linear.bias) \n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.linear(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class implements at least two methods: `__init__()` and `forward()`.\n",
        "\n",
        "### `__init()__`\n",
        "\n",
        "\n",
        "\n",
        "In this function we declare all the layers we want to use. For the logistic regression model we need one linear layer `torch.nn.Linear`, which is a single layer feed forward network with `input_dim` inputs and `output_dim` outputs.\n",
        "\n",
        "The method `__init__()` is called when an instance of our class `LogisticRegression` is created."
      ],
      "metadata": {
        "id": "Fj8ygxXdET-y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsEKA3A_ec01"
      },
      "outputs": [],
      "source": [
        "# Two inputs x_1 and x_2\n",
        "input_dim = 2  \n",
        "# Single binary output \n",
        "output_dim = 1 \n",
        "\n",
        "model = LogisticRegression(input_dim, output_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now created a `LogisticRegression` instance called `model` that has three  modelparameters: one for each feature `x_1` and `x_2` (aka weights) and one bias (`x_0`). \n",
        "\n",
        "As this class inherits all functionality of the `totch.nn.Module` class, we can now, for instance, call the inherited `.state_dict()` method to get the state (what the model contains) of our model."
      ],
      "metadata": {
        "id": "pXeeaE4wFoTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "id": "xi1jwgIEFarB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can access these modelparameters as follows."
      ],
      "metadata": {
        "id": "eQxTz7yd4ZMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.linear.weight"
      ],
      "metadata": {
        "id": "rMIHDMYV4O_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used `torch.nn.init.uniform_()` to set the initial values of the modelparameters by random sampling from a normal disribution with mean equal to zero and standard deviation equal to one.\n",
        "\n",
        "The following code plots the initial decision boundary of our model. "
      ],
      "metadata": {
        "id": "Yy5CGXVM8jaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(model, X_train, y_train)"
      ],
      "metadata": {
        "id": "TsprpkRanp-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKdLN7nuheb"
      },
      "source": [
        "### `forward()`\n",
        "\n",
        "When we pass data to our model, it'll go through the model's `forward()` method and produce a result using the computation we've defined. \n",
        "\n",
        "In this method we pass a feature vector `x` trough the linear layer. \n",
        "\n",
        "Notice that the result is not passed trough a sigmoid function to output a class probability, as is the case in a logistic regression model. This is because we will use `BCEWithLogitsLoss` as the loss function (see below). This loss function combines the sigmoid function and the binary cross entropy loss in one single class (this is numerically more stable than using a plain sigmoid followed by a binary cross entropy loss).\n",
        "\n",
        "Let's compute predictions for the first 10 feature vectors in the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.inference_mode(): \n",
        "    predictions = model(X_test[:10])\n",
        "\n",
        "predictions"
      ],
      "metadata": {
        "id": "-BE18lvMKx5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we make predictions with a model we first set the model in evaluation mode. We do this by calling `model.eval()` (which will notify all your layers that you are in eval mode, we will see later why this is important) and `torch.inference_mode()` (which turns off a bunch of things (like gradient tracking, which is necessary for training but not for inference) to make **forward-passes** (data going through the `forward()` method) faster).\n",
        "\n",
        "The `predictions` are raw (non-normalized) predictions, called logits, that a classification our model generates with the `forward()` method.\n",
        "\n",
        "The predictions are passed through a sigmoid function to obtain class probabilities."
      ],
      "metadata": {
        "id": "5kMjNObvLhc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = torch.sigmoid(predictions)\n",
        "predictions"
      ],
      "metadata": {
        "id": "USWz78-n7m0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `round()` method converts these probabilities to (binary) classes."
      ],
      "metadata": {
        "id": "sW_yisicMMCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = torch.round(predictions)\n",
        "predictions"
      ],
      "metadata": {
        "id": "1gsvyky87xXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we work with tensors, the model outputs are returned as an array of (1-dimensional) arrays."
      ],
      "metadata": {
        "id": "GHOzlzIbLdh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions.shape)"
      ],
      "metadata": {
        "id": "sREjmDPELm3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `squeeze()` method reshape this tensor to a 1-dimensional array by removing all dimensions with size 1."
      ],
      "metadata": {
        "id": "L28W91tyNhhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = torch.squeeze(predictions)\n",
        "\n",
        "predictions"
      ],
      "metadata": {
        "id": "LQ59eV4ZNKsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now convert the tensor back to numpy using the `detach()` and the `numpy()` methods."
      ],
      "metadata": {
        "id": "w5POsW0_N0A6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = predictions.detach().numpy()\n",
        "\n",
        "predictions"
      ],
      "metadata": {
        "id": "zZ6_QqNkNSKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Tensor.detach() method is used to detach a tensor from the current computational graph (more about this later). \n",
        "\n",
        "We also need to detach a tensor when we need to move the tensor from GPU to CPU.\n",
        "\n",
        "We can now use Scikit-learn to compute evaluation metrics, e.g. the accuracy."
      ],
      "metadata": {
        "id": "Jeb4dzdfLunz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ITlZgU5ec02"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "with torch.inference_mode(): \n",
        "    predictions = model(X_test)\n",
        "\n",
        "predictions = torch.squeeze(torch.round(torch.sigmoid(predictions)))\n",
        "predictions = predictions.detach().numpy()\n",
        "\n",
        "print(\"test set accuracy: {}\".format(accuracy_score(y_test,predictions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD8pnhJUyZUT"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "Our model is making predictions using random modelparameter values.\n",
        "\n",
        "\n",
        "To fit the modelparameters to the training data we'll need to add a **loss function** and an **optimizer**. The loss function measures how wrong the model predictions are compared to the true labels. The optimizer tells the model how to update its modelparameters to best lower the loss.\n",
        "\n",
        "As our task is a two-class classification task we use a binary cross entropy loss `BCEWithLogitsLoss` that implicitly also applies a sigmoid normalization of the logits.\n",
        "\n",
        "For the optimizer we use stochastic gradient descent (SGD), `torch.optim.SGD(params, lr)` where:\n",
        "\n",
        "* `params` are the modelparameters we want to optimize\n",
        "* `lr` is the **learning rate** you'd like the optimizer to update the modelparameters at\n",
        "\n",
        "First we put the model back in training mode (to undo `model.eval()`)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()"
      ],
      "metadata": {
        "id": "GZARX3z3ugyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3T7hpNPec03"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "#the loss function\n",
        "loss_func = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "#the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFcKCsPcRfnA"
      },
      "source": [
        "Now we've got a loss function and an optimizer, it's now time to create a **training loop** (and **validation loop**).\n",
        "\n",
        "For the training loop, we have to code the following steps:\n",
        "\n",
        "1. Forward pass: the model goes through all of the training data once, performing its `forward()` function calculations.\n",
        "\n",
        "2. Calculate the loss: the model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are.\n",
        "3. Zero the gradients: the optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step.\n",
        "4. Perform backpropagation on the loss: computes the gradient of the loss with respect to every modelparameter \n",
        "5. Update the optimizer (**gradient descent**): update the modelparameter values with respect to the loss gradients.\n",
        "\n",
        "These 5 steps are executed `num_epochs` times, where in each **epoch** the full train set is passed through the network.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1DfhyJ7ec03"
      },
      "outputs": [],
      "source": [
        "#number of times we iterate trough the train set\n",
        "num_epochs = 8000\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    #step 1\n",
        "    predictions_train = torch.squeeze(model(X_train)) \n",
        "\n",
        "    #step 2\n",
        "    loss = loss_func(predictions_train, y_train) \n",
        "\n",
        "    #step 3\n",
        "    optimizer.zero_grad() \n",
        "\n",
        "    #step 4\n",
        "    loss.backward() \n",
        "\n",
        "    #step 5\n",
        "    optimizer.step() # Updates weights and biases with the optimizer (SGD)\n",
        "        \n",
        "    if epoch % 500 == 0:    \n",
        "      print(\"training loss: {}\".format(loss))    \n",
        "      model.eval()\n",
        "      with torch.inference_mode(): \n",
        "        predictions_val = torch.squeeze(torch.round(torch.sigmoid(model(X_val)))).detach().numpy()\n",
        "        print(\"validation accuracy: {}\".format(accuracy_score(y_val,predictions_val)))\n",
        "      model.train()\n",
        "      plot_decision_boundary(model, X_train, y_train)\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing predictions and evaluating the model\n",
        "\n",
        "After fitting the modelparameters (and optimizing the hyperparameters on the validation set) we can compute the prediction accuracy on the test set. "
      ],
      "metadata": {
        "id": "e0MESO4WgNQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.inference_mode(): \n",
        "    predictions_test = model(X_test)\n",
        "\n",
        "predictions_test = torch.round(torch.sigmoid(torch.squeeze(predictions_test))).detach().numpy()\n",
        "\n",
        "print(\"test set accuracy: {}\".format(accuracy_score(y_test,predictions_test)))"
      ],
      "metadata": {
        "id": "X1saE3rqbrS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_decision_boundary(model, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "h5F0-4MKswTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdAGcH2aec05"
      },
      "source": [
        "## Saving (and loading) the model\n",
        "\n",
        "The recommended way for saving a model for inference (making predictions) is by saving the modelparameter values in `state_dict()`.\n",
        "\n",
        "We call `torch.save(obj, f)` where `obj` is the target model's `state_dict()` and `f` is the filename of where to save the model.\n",
        "\n",
        "It's common convention for PyTorch saved models or objects to end with `.pt` or `.pth`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsQhY2S2jv90"
      },
      "outputs": [],
      "source": [
        "model_filename = \"model_logistic_regression.pth\"\n",
        "torch.save(obj=model.state_dict(), f=model_filename) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFQpRoH5ec06"
      },
      "source": [
        "To load a model, we first load the `state_dict()` with `torch.load()` and then pass that `state_dict()` to a new instance of our model (which is a subclass of `nn.Module`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xnh3cFDec06"
      },
      "outputs": [],
      "source": [
        "loaded_model = LogisticRegression(input_dim, output_dim)\n",
        "\n",
        "loaded_model.load_state_dict(torch.load(f=model_filename))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK8PRtY7Qgpz"
      },
      "source": [
        "Now to test our loaded model, let's perform inference with it (make predictions) on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ps-AuJqkec06"
      },
      "outputs": [],
      "source": [
        "loaded_model.eval()\n",
        "\n",
        "with torch.inference_mode():\n",
        "    loaded_model_preds = loaded_model(X_test) "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "01_pytorch_workflow.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "3fbe1355223f7b2ffc113ba3ade6a2b520cadace5d5ec3e828c83ce02eb221bf"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}