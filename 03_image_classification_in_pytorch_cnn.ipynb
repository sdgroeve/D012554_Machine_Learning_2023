{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sdgroeve/D012554_Machine_Learning_2023/blob/main/03_image_classification_in_pytorch_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "request = requests.get(\"https://raw.githubusercontent.com/sdgroeve/D012554_Machine_Learning_2023/main/utils/utils.py\")\n",
        "with open(\"utils.py\", \"wb\") as f:\n",
        "  f.write(request.content)\n",
        "\n",
        "from utils import plot_decision_boundary"
      ],
      "metadata": {
        "id": "3lBi6JV6xhXZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!pip install tqdm\n",
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU4n2UT6ZsWA",
        "outputId": "a5b7646c-a8ae-4fe1-d075-d360b4913896",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torchmetrics-1.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgYkrRCRec0r"
      },
      "source": [
        "# 3. Image Classification in PyTorch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZT_ikDC-ec0w",
        "outputId": "6dfcd014-d311-4ae2-bebc-988a0095258a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.2.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "torch.manual_seed(46)\n",
        "\n",
        "# Check PyTorch version\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci_-geIdec0w"
      },
      "source": [
        "## Preparing the data\n",
        "\n",
        "We will use [`torchvision`](https://pytorch.org/vision/stable/index.html) to load our dataset. This library contains datasets, model architectures and image transformations often used for computer vision prediction tasks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import torchvision\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\n",
        "print(f\"torchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZo_wyZRbMmd",
        "outputId": "1fc3f7af-b843-4aa4-dabc-3efb1fdda235"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torchvision version: 0.17.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will train our model on the [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist) dataset, made by Zalando Research, that contains grayscale images of 10 different kinds of clothing.\n",
        "\n",
        "To download it, we provide the following parameters:\n",
        "* `root: str` - which folder do you want to download the data to?\n",
        "* `train: Bool` - do you want the training or test split?\n",
        "* `download: Bool` - should the data be downloaded?\n",
        "* `transform: torchvision.transforms` - what transformations would you like to do on the data?\n",
        "* `target_transform` - you can transform the targets (labels) if you like too.\n",
        "\n",
        "Let's load the data using torchvision."
      ],
      "metadata": {
        "id": "VFZFvc4kdVtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train set\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True, # get training data\n",
        "    download=True,\n",
        "    transform=ToTensor(), # images come as PIL format, we want to turn them into Torch tensors\n",
        "    target_transform=None\n",
        ")\n",
        "\n",
        "# test set\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # get test data\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ],
      "metadata": {
        "id": "-emcG7FnbWjB",
        "outputId": "60d19a84-f04f-4320-cf77-b2a946d98af1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 12575580.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 205229.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3942138.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 5941738.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 60.000 training images and 10.000 testing images."
      ],
      "metadata": {
        "id": "BJ7J5Z4MVCiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0V1_9EuU_lu",
        "outputId": "7c8e872e-2fd4-42fa-e60a-93ff76df1aee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the classes:"
      ],
      "metadata": {
        "id": "Gvq4ZGKCVvco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_data.classes\n",
        "print(len(class_names))\n",
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDL13CqaVuXx",
        "outputId": "cad5b08c-6537-4dc8-8a23-945c4b1a18b3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['T-shirt/top',\n",
              " 'Trouser',\n",
              " 'Pullover',\n",
              " 'Dress',\n",
              " 'Coat',\n",
              " 'Sandal',\n",
              " 'Shirt',\n",
              " 'Sneaker',\n",
              " 'Bag',\n",
              " 'Ankle boot']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code splits the training data into a training data and validation data."
      ],
      "metadata": {
        "id": "YvApnWeqVVgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, validation_data = torch.utils.data.random_split(train_data, [50000, 10000])"
      ],
      "metadata": {
        "id": "ZPUueophU1Bc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the first image (data point) in the train set."
      ],
      "metadata": {
        "id": "_EoNPyLR7Lqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = train_data[0]\n",
        "image, label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AHLTkPu7TCn",
        "outputId": "ebf6da33-e461-42db-84a1-7b85d3e31681"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0078, 0.0000, 0.0588, 0.2588, 0.0000, 0.0000, 0.0667,\n",
              "           0.2431, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0000, 0.0157, 0.3216, 0.0000, 0.0000, 0.0667,\n",
              "           0.2627, 0.0000, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.5490, 0.6196, 0.4824, 0.6431,\n",
              "           0.0667, 0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0000, 0.0000, 0.8902, 1.0000, 0.9686, 0.9569,\n",
              "           0.0745, 0.0000, 0.0078, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.5804, 0.9490, 0.8471, 0.7961,\n",
              "           0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.2980, 0.9647, 0.8745, 0.7608,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.5059, 0.9686, 0.8667, 0.9255,\n",
              "           0.2667, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000, 0.7843, 0.9765, 0.8157, 0.8824,\n",
              "           0.2000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0039, 0.0000, 0.0000, 0.9882, 0.9255, 0.8392, 0.8667,\n",
              "           0.2863, 0.0000, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0118, 0.0000, 0.1961, 1.0000, 0.9255, 0.8745, 0.8902,\n",
              "           0.7176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.3176, 0.9647, 0.9176, 0.8863, 0.8706,\n",
              "           0.7176, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0039, 0.0000, 0.0000, 0.3333, 1.0000, 0.9059, 0.8863, 1.0000,\n",
              "           0.5333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0078, 0.0000, 0.0275, 0.2824, 0.9216, 0.9059, 0.8706, 1.0000,\n",
              "           0.2353, 0.1098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0039, 0.0000, 0.0588, 0.2510, 0.7922, 0.9294, 0.8941, 0.9804,\n",
              "           0.0549, 0.3804, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0588, 0.4039, 0.6000, 0.9451, 0.9843, 0.6745,\n",
              "           0.1882, 0.6235, 0.0000, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0039, 0.0000, 0.1098, 0.6196, 0.4588, 1.0000, 1.0000, 0.3373,\n",
              "           0.4706, 0.6431, 0.0000, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0157, 0.0000, 0.2745, 0.5137, 0.5098, 1.0000, 0.9137, 0.2706,\n",
              "           0.4588, 0.6706, 0.0000, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0118, 0.0000, 0.6314, 0.2706, 0.6549, 0.9686, 0.9176, 0.4078,\n",
              "           0.3569, 0.8275, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.7765, 0.0784, 0.8235, 0.9020, 0.9451, 0.6235,\n",
              "           0.3098, 0.9961, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.8863, 0.0235, 0.9686, 0.8588, 0.9412, 0.7765,\n",
              "           0.3451, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.1216, 0.8039, 0.0784, 1.0000, 0.8118, 0.9765, 0.8431,\n",
              "           0.3216, 1.0000, 0.1569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.4471, 0.6510, 0.1961, 1.0000, 0.8118, 0.9961, 0.8824,\n",
              "           0.3333, 1.0000, 0.4235, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.8902, 0.4667, 0.3843, 1.0000, 0.8196, 1.0000, 0.8902,\n",
              "           0.3882, 1.0000, 0.7569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 1.0000, 0.2667, 0.6353, 1.0000, 0.8196, 1.0000, 0.8431,\n",
              "           0.4549, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.3529, 1.0000, 0.1529, 0.8157, 1.0000, 0.8392, 1.0000, 0.7804,\n",
              "           0.4784, 0.9647, 1.0000, 0.2980, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.5176, 1.0000, 0.3137, 0.9020, 0.9451, 0.8941, 1.0000, 0.8353,\n",
              "           0.5059, 0.9686, 0.8745, 0.7843, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 1.0000, 0.4039, 0.9961, 1.0000, 1.0000, 1.0000, 0.9529,\n",
              "           0.7725, 1.0000, 1.0000, 0.9020, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.4902, 0.0000, 0.2510, 0.9490, 0.5176, 0.0000, 0.0000,\n",
              "           0.6039, 0.9216, 0.4745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "           0.0000, 0.0000, 0.0000, 0.0000]]]),\n",
              " 3)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each image is a tensor with the following shape."
      ],
      "metadata": {
        "id": "fxAv20e98ZS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08DYvwyA7cVr",
        "outputId": "dac84b1a-0a46-46d1-9da0-7a28989ee190"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape of the image tensor is `[1, 28, 28]` or more specifically:\n",
        "\n",
        "```\n",
        "[color_channels=1, height=28, width=28]\n",
        "```\n",
        "\n",
        "Having `color_channels=1` means the image is grayscale."
      ],
      "metadata": {
        "id": "flEpM21P8dsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the first image in the train set."
      ],
      "metadata": {
        "id": "XTpz09s6JC0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import matplotlib.pyplot as plt\n",
        "\n",
        "#image, label = train_data[0]\n",
        "\n",
        "#plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "#plt.title(class_names[label])"
      ],
      "metadata": {
        "id": "wLAkhtiRImaP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming train_data is your dataset\n",
        "image, label = train_data[1] # Get the first image and its label\n",
        "\n",
        "# Permute the tensor from [C, H, W] to [H, W, C]\n",
        "image = image.permute(1, 2, 0)\n",
        "\n",
        "plt.imshow(image)\n",
        "plt.title(f\"Label: {label}\")\n",
        "plt.axis('off') # Hide the axes\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "6wyzIPCgHvfb",
        "outputId": "81d5cafd-46c3-451c-a19e-b6a8e493defe"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up till now we have updated the modelparameters once in each epoch. For large dataset this is highly inefficient.\n",
        "\n",
        "It is better to partition the train set into smaller chucks of data, called **batches** (or **minibatches** as this approach works best for very small batch sizes), and update the modelparameters after each batch.  \n",
        "\n",
        "A minibatch size of [32 data points](https://twitter.com/ylecun/status/989610208497360896?s=20&t=N96J_jotN--PYuJk2WcjMw) is a good place to start for most tasks.\n",
        "\n",
        "The PyTorch `DataLoader` class makes using minibatches really easy."
      ],
      "metadata": {
        "id": "xmT4PrIGQAWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "train_dataloader = DataLoader(train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_dataloader = DataLoader(validation_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "_aSQcQfvSG5t"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eFsorRHec00"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "Since each image is a 2-dimensional (3 if you count the channel), we first **flatten** the image to just 1 dimension to prepare the input for the input layer of our neural network, i.e. each pixel is one feature."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a flatten layer\n",
        "flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n",
        "\n",
        "# Get a single sample\n",
        "x, y = train_data[0]\n",
        "\n",
        "# Flatten the sample\n",
        "output = flatten_model(x) # perform forward pass\n",
        "\n",
        "# Print out what happened\n",
        "print(f\"Shape before flattening: {x.shape} -> [color_channels, height, width]\")\n",
        "print(f\"Shape after flattening: {output.shape} -> [color_channels, height*width]\")\n",
        "\n",
        "# Try uncommenting below and see what happens\n",
        "#print(x)\n",
        "#print(output)"
      ],
      "metadata": {
        "id": "2lk2KEQNYpMx",
        "outputId": "4152468c-b6dc-4ca6-fd6e-bacd02d1e719",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape before flattening: torch.Size([1, 28, 28]) -> [color_channels, height, width]\n",
            "Shape after flattening: torch.Size([1, 784]) -> [color_channels, height*width]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "jhcUJBFuec00"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        num_neurons_layer_2 = 10\n",
        "\n",
        "        self.layer_1 = nn.Linear(in_features=input_dim, out_features=num_neurons_layer_2)\n",
        "        self.layer_2 = nn.Linear(in_features=num_neurons_layer_2, out_features=output_dim)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = self.relu(self.layer_1(x))\n",
        "        x = self.layer_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `__init()__`\n",
        "\n",
        "Our neural network has two linear layers. The first layer `layer_1` has `input_dim` (the number of features in our dataset) input features that form the **input layer**. It has `num_neurons_layer_2` output features that form the **hidden layer** where these features are typically called **hidden neurons**.\n",
        "\n",
        "The second layer `layer_2` has `num_neurons_layer_2` input features (neurons) and `output_dim` (which equals to 1 for two-class classification) output features, the **output layer**.\n",
        "\n",
        "An example of this model architecture with `num_neurons_layer_2 = 6` can be seen [here](https://playground.tensorflow.org/#activation=sigmoid&batchSize=30&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=6&seed=0.86658&showTestData=false&discretize=false&percTrainData=70&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false).\n",
        "\n",
        "We will use the Rectified Linear Unit (ReLU) activation function in the hidden layer. In the output layer we use the sigmoid function (through `BCEWithLogitsLoss`, so we not to explicitly apply the sigmoid function during inference (see notebook about logistic regression)).\n",
        "\n",
        "Next, we create an instance of the class `NeuralNetwork`."
      ],
      "metadata": {
        "id": "Fj8ygxXdET-y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CsEKA3A_ec01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80026cb9-052b-450d-c2bd-e5ea43ff2aab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('layer_1.weight',\n",
              "              tensor([[-0.0177,  0.0270, -0.0024,  ..., -0.0121, -0.0204, -0.0316],\n",
              "                      [-0.0291, -0.0139, -0.0153,  ..., -0.0259,  0.0311, -0.0356],\n",
              "                      [ 0.0248,  0.0198, -0.0184,  ..., -0.0233,  0.0338,  0.0035],\n",
              "                      ...,\n",
              "                      [ 0.0235,  0.0024,  0.0184,  ...,  0.0322, -0.0289, -0.0077],\n",
              "                      [-0.0041,  0.0162, -0.0284,  ..., -0.0315, -0.0167,  0.0337],\n",
              "                      [ 0.0183, -0.0258,  0.0028,  ..., -0.0283,  0.0045, -0.0312]])),\n",
              "             ('layer_1.bias',\n",
              "              tensor([-0.0047, -0.0152,  0.0237, -0.0074, -0.0107, -0.0118,  0.0278,  0.0234,\n",
              "                       0.0116, -0.0284])),\n",
              "             ('layer_2.weight',\n",
              "              tensor([[ 0.1242, -0.0347, -0.1303, -0.2768, -0.0494,  0.1606,  0.2478, -0.2423,\n",
              "                       -0.0204,  0.1201],\n",
              "                      [-0.0994, -0.1074,  0.1694, -0.0305,  0.2804, -0.1191, -0.1110,  0.0884,\n",
              "                       -0.0579,  0.0850],\n",
              "                      [-0.0342, -0.1113,  0.2962, -0.2599, -0.0782, -0.0790, -0.1353,  0.1447,\n",
              "                        0.1713,  0.0304],\n",
              "                      [ 0.0360, -0.1321, -0.1316,  0.2884, -0.0467,  0.0326,  0.2355, -0.2135,\n",
              "                        0.3008, -0.1419],\n",
              "                      [-0.0183, -0.2338, -0.0064,  0.2393, -0.0605, -0.0311,  0.0644,  0.2829,\n",
              "                        0.2943,  0.2133],\n",
              "                      [ 0.2266, -0.1659, -0.2535, -0.2042,  0.2902, -0.0115,  0.2261,  0.0920,\n",
              "                       -0.1542, -0.2063],\n",
              "                      [-0.0314,  0.1750,  0.0220, -0.1185, -0.0148,  0.1421, -0.1180, -0.2455,\n",
              "                        0.0663, -0.1671],\n",
              "                      [-0.2474, -0.1397, -0.1234,  0.1740,  0.2727, -0.0469, -0.0906, -0.2713,\n",
              "                       -0.1438, -0.2788],\n",
              "                      [-0.2630,  0.0286,  0.1605,  0.3050,  0.0290,  0.0395, -0.0375,  0.0255,\n",
              "                        0.2482, -0.2261],\n",
              "                      [-0.2146, -0.0450,  0.3074, -0.0226,  0.0405,  0.0194, -0.0640,  0.1546,\n",
              "                        0.1058,  0.2226]])),\n",
              "             ('layer_2.bias',\n",
              "              tensor([ 0.3126,  0.2122, -0.2959, -0.1158,  0.0028,  0.2550,  0.1222,  0.2078,\n",
              "                       0.2714, -0.0154]))])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Two inputs x_1 and x_2\n",
        "input_dim = 784\n",
        "# Single binary output\n",
        "output_dim = 10\n",
        "\n",
        "# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\n",
        "model = NeuralNetwork(input_dim, output_dim)\n",
        "\n",
        "model.state_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDKdLN7nuheb"
      },
      "source": [
        "### `forward()`\n",
        "\n",
        "The `forward()` method applies the neural network to the provided feature vectors. Here we see that the data is first passed through `layer_1`, then through the ReLU activations that then pass through `layer_2`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(device)\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "oVhnxzf1Q1JX",
        "outputId": "c7a3ef79-f085-4e6c-d1fc-2609b8f4faa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (layer_1): Linear(in_features=784, out_features=10, bias=True)\n",
              "  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchmetrics\n",
        "\n",
        "metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)"
      ],
      "metadata": {
        "id": "gsHsfzU9QqyF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD8pnhJUyZUT"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "We use `BCEWithLogitsLoss` as the loss function and SGD, `torch.optim.SGD(params, lr)` as the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "P3T7hpNPec03"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.01\n",
        "\n",
        "#the loss function\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "#the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFcKCsPcRfnA"
      },
      "source": [
        "Now we can create and run our training and validation loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1DfhyJ7ec03",
        "outputId": "901e3fa2-b251-4a6c-dbc0-5da6fbad44b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 1.07752 | Validation loss: 0.71432, Validation acc: 0.75%\n",
            "Train loss: 0.63203 | Validation loss: 0.58895, Validation acc: 0.79%\n",
            "Train loss: 0.55255 | Validation loss: 0.54369, Validation acc: 0.81%\n",
            "Train loss: 0.51416 | Validation loss: 0.51294, Validation acc: 0.82%\n",
            "Train loss: 0.49030 | Validation loss: 0.51434, Validation acc: 0.82%\n",
            "Train loss: 0.47343 | Validation loss: 0.49113, Validation acc: 0.83%\n",
            "Train loss: 0.46116 | Validation loss: 0.47357, Validation acc: 0.83%\n",
            "Train loss: 0.45149 | Validation loss: 0.46696, Validation acc: 0.84%\n",
            "Train loss: 0.44458 | Validation loss: 0.46921, Validation acc: 0.84%\n",
            "Train loss: 0.43852 | Validation loss: 0.46844, Validation acc: 0.84%\n"
          ]
        }
      ],
      "source": [
        "#number of times we iterate trough the train set\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      #step 1\n",
        "      predictions_train = model(X)\n",
        "\n",
        "      #step 2\n",
        "      loss = loss_func(predictions_train, y)\n",
        "      train_loss += loss\n",
        "\n",
        "      #step 3\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #step 4\n",
        "      loss.backward()\n",
        "\n",
        "      #step 5\n",
        "      optimizer.step()\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.inference_mode():\n",
        "        for X, y in validation_dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            y_pred_logits = model(X)\n",
        "            y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "            # 2. Calculate loss (accumatively)\n",
        "            val_loss += loss_func(y_pred_logits, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            metric.update(y_pred_logits, y)\n",
        "\n",
        "        val_loss /= len(validation_dataloader)\n",
        "\n",
        "    val_acc = metric.compute()\n",
        "    metric.reset()\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"Train loss: {train_loss:.5f} | Validation loss: {val_loss:.5f}, Validation acc: {val_acc:.2f}%\")\n",
        "\n",
        "    model.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_acc = 0\n",
        "with torch.inference_mode():\n",
        "    for X, y in test_dataloader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred_logits = model(X)\n",
        "        #y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "        # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "        metric.update(y_pred_logits, y)\n",
        "\n",
        "model.train()\n",
        "\n",
        "test_acc = metric.compute()\n",
        "\n",
        "## Print out what's happening\n",
        "print(f\"\\nTest acc: {test_acc:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "8CS7DuRIwj8m",
        "outputId": "5647f788-2a44-4b42-c52e-100a1ac5937c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "mat1 and mat2 shapes cannot be multiplied (32x490 and 640x10)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-bbad3ce6c054>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# 1. Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0my_pred_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-ea51e0243592>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x490 and 640x10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Model architecture copying TinyVGG from:\n",
        "    https://poloclub.github.io/cnn-explainer/\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3, # how big is the square that's going over the image?\n",
        "                      stride=1, # default\n",
        "                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units,\n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2) # default stride value is same as kernel_size\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            # Where did this in_features shape come from?\n",
        "            # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "            nn.Linear(in_features=hidden_units*7*7,\n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        # print(x.shape)\n",
        "        x = self.block_2(x)\n",
        "        # print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        # print(x.shape)\n",
        "        return x"
      ],
      "metadata": {
        "id": "kZtwC9wx6t46"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN(input_shape=1,\n",
        "    hidden_units=10,\n",
        "    output_shape=len(class_names)).to(device)"
      ],
      "metadata": {
        "id": "T6w9ATp37VAz"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "\n",
        "#the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "Ee3_pM2A7673"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of times we iterate trough the train set\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "      X = X.to(device)\n",
        "      y = y.to(device)\n",
        "      #step 1\n",
        "      predictions_train = model(X)\n",
        "\n",
        "      #step 2\n",
        "      loss = loss_func(predictions_train, y)\n",
        "      train_loss += loss\n",
        "\n",
        "      #step 3\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #step 4\n",
        "      loss.backward()\n",
        "\n",
        "      #step 5\n",
        "      optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.inference_mode():\n",
        "        for X, y in validation_dataloader:\n",
        "            X = X.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            y_pred_logits = model(X)\n",
        "            y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "            # 2. Calculate loss (accumatively)\n",
        "            val_loss += loss_func(y_pred_logits, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            metric.update(y_pred_logits, y)\n",
        "\n",
        "        val_loss /= len(validation_dataloader)\n",
        "\n",
        "    val_acc = metric.compute()\n",
        "\n",
        "    metric.reset()\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"Train loss: {train_loss:.5f} | Validation loss: {val_loss:.5f}, Validation acc: {val_acc:.2f}%\")\n",
        "\n",
        "    model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h_8lqM77fl-",
        "outputId": "7266a3fb-14b8-4b56-85e2-92212b326fe3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 995.41534 | Validation loss: 0.45579, Validation acc: 0.83%\n",
            "Train loss: 585.63019 | Validation loss: 0.43552, Validation acc: 0.84%\n",
            "Train loss: 536.62610 | Validation loss: 0.33423, Validation acc: 0.88%\n",
            "Train loss: 507.17398 | Validation loss: 0.35611, Validation acc: 0.87%\n",
            "Train loss: 485.53690 | Validation loss: 0.33153, Validation acc: 0.88%\n",
            "Train loss: 470.08884 | Validation loss: 0.33239, Validation acc: 0.88%\n",
            "Train loss: 459.91019 | Validation loss: 0.32621, Validation acc: 0.88%\n",
            "Train loss: 447.30164 | Validation loss: 0.31801, Validation acc: 0.89%\n",
            "Train loss: 438.64334 | Validation loss: 0.31470, Validation acc: 0.89%\n",
            "Train loss: 430.28021 | Validation loss: 0.39506, Validation acc: 0.85%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing predictions and evaluating the model\n"
      ],
      "metadata": {
        "id": "e0MESO4WgNQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "test_acc = 0\n",
        "with torch.inference_mode():\n",
        "    for X, y in test_dataloader:\n",
        "        X = X.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred_logits = model(X)\n",
        "        #y_pred = torch.softmax(y_pred_logits.squeeze(), dim=0)\n",
        "\n",
        "        # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "        metric.update(y_pred_logits, y)\n",
        "\n",
        "model.train()\n",
        "\n",
        "test_acc = metric.compute()\n",
        "\n",
        "## Print out what's happening\n",
        "print(f\"\\nTest acc: {test_acc:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "X1saE3rqbrS3",
        "outputId": "add05f30-f110-401d-f11f-6ac42223488a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test acc: 0.85%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MkSk6FTvath3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "01_pytorch_workflow.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "3fbe1355223f7b2ffc113ba3ade6a2b520cadace5d5ec3e828c83ce02eb221bf"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}